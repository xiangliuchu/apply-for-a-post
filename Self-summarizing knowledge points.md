# 自总结知识点
## springMVC中不同注解的作用   详细见：[springMVC](D:\Java\java50th\java50-course-materials\03-JavaEE&Spring框架\02-笔记\Day22-26-SpringMVC.md)

### @RequestMapping : 用于请求路径和处理方法的注解 ， 主要实现一些限定

```java
1.★★★value: url路径映射，窄化请求； 如： @RequestMapping(value={"hello"})//映射URL → localhost:8080/hello
2.method: 请求方法的限定；      如： @RequestMapping(value = "get",method = RequestMethod.GET)
3.params: 请求参数的限定：      如： @RequestMapping(value = "login",params = {"username","password"})
4.headers: 请求头的限定；       如： @RequestMapping(value = "limit",headers = {"abc","def"})
5.consumes: content-type这个请求头的限定→ 这个请求头的含义：请求携带的 正文的类型（请求体） 如一个jpg文件：image/jpeg如文本：text/html 如json：application/json
6.produces: 限定的是Accept这个请求头的值 → 这个请求的含义 → 客户端希望接收到的服务端响应的正文类型   比如一个jpg文件：image/jpeg  比如文本：text/html  比如json：application/json 
```

### 放在方法的形参中的一些注解：

- 请求URL的信息 → @PathVariable     获得请求url中占位符所在的值

  ```java
  //localhost:8080/path/songge
  //@PathVariable → 获得请求URL的一部分值 → 在@RequestMapping的value属性写占位符{}
  // 获得指定占位符位置的值给到所对应的形参 → 形参通过注解接收指定占位符位置的值
  // 通过这个注解，就可以把一部分请求参数写到URL中，比如豆瓣、CSDN
  @RequestMapping("path/{username}")
  public BaseRespVo path(@PathVariable("username")String name) {
      System.out.println("name = " + name);
      return BaseRespVo.ok();
  }
  ```

  

- 请求参数信息 → @RequestParam

- 请求头信息 → @RequestHeader

- Cookie信息 → @CookieValue

- Session信息 → @SessionAttribute

### @ResponseBody 如果写在类上，意味着当前类下所有的方法响应的都是字符串或Json字符串 （附 ：@RestController）

引申注解：<span style='color:yellow;background:red;font-size:文字大小;font-family:字体;'>**@RestController **</span>**= @Controller + @ResponseBody**

@RestController的用途：

1. **自动转换为响应体格式**：@RestController 注解会自动将方法返回的对象转换为JSON或XML格式的响应体，根据请求的 Accept 头部信息确定响应的格式。
2. **常用于构建RESTful API**：@RestController 注解通常用于构建RESTful风格的API，可以方便地处理HTTP请求和返回结构化数据。
3. **与 @RequestMapping 结合使用**：@RestController 注解可以与 @RequestMapping 注解结合使用，以定义控制器类或方法的请求映射路径和处理逻辑。
4. **支持常见的HTTP方法**：@RestController 注解支持常见的HTTP方法，如GET、POST、PUT、DELETE等，通过不同的请求映射路径和方法来处理不同的请求。
5. **无需额外的视图解析器**：由于 @RestController 注解主要用于返回数据而不是视图，因此不需要额外配置视图解析器。

### @RequestBody，表示接收时是json

**注意： 方法的形参列表只能写一个@RequestBody，因为HTTP请求的请求体只能被读取一次，如果实在要读取多个参数，可以将他们封装成一个更复杂的对象。**

Json请求参数仍然是在形参中进行接收，可以使用以下几种类型来接收

- **String**
- **引用类型对象**
- **Map**

**形参前需要增加一个注解@RequestBody**

> @RequestBody和@ResponseBody这两个注解都是和Json打交道的时候使用的注解
>
> 接收的时候用的是@RequestBody
>
> 响应的时候用的是@ResponseBody



###  @Autowired用于组件的注入，如方法注入、成员变量等     详细见：[SpringIOC](D:\Java\java50th\java50-course-materials\03-JavaEE&Spring框架\02-笔记\Day18-20-Spring-IOC.md)

###  @Qualifier

**`@Qualifier` 是一个用于解决依赖注入中多个候选对象的歧义性的Spring框架注解。它可以与 `@Autowired` 注解一起使用，通过指定限定符来精确选择要注入的bean对象。**

当有多个类型匹配的候选bean对象时，Spring无法确定要注入哪个对象。这时，可以使用 `@Qualifier` 注解结合自定义的限定符来明确指定要注入的bean对象。

以下是关于 `@Qualifier` 注解的一些要点：

1. **限定符概念**：限定符是一个用于标识bean对象的标签，可以自定义命名。通过使用 `@Qualifier` 注解，可以将一个限定符与被注入的bean对象相关联。

2. **与 `@Autowired` 结合使用**：`@Qualifier` 注解通常与 `@Autowired` 注解一起使用，以指定要注入的bean对象的限定符。

3. **按照限定符匹配**：当存在多个匹配的bean对象时，Spring会尝试匹配被 `@Qualifier` 注解指定的限定符与bean对象的限定符进行匹配，从而确定要注入的对象。

4. **自定义限定符**：可以通过自定义注解或者使用字符串来定义限定符。自定义注解需要使用元注解 `@Qualifier` 进行标注，以确保其被用作限定符。

以下是一个使用 `@Qualifier` 注解的示例：

```java
@Service
public class UserService {

    @Autowired
    @Qualifier("userDaoImpl")
    private UserDao userDao;

    // ...
}
```

在上述示例中，`UserService` 类使用 `@Autowired` 注解将 `UserDao` 对象注入到 `userDao` 字段中。通过在 `@Qualifier` 注解中指定限定符 `"userDaoImpl"`，可以确保注入的是具有相应限定符的 `UserDao` bean 对象。

总而言之，`@Qualifier` 注解是Spring框架中用于解决依赖注入中多个候选对象歧义性的关键注解之一。它允许精确指定要注入的bean对象，以确保正确的依赖关系被建立。

### @PostConstruct

`@PostConstruct` 是一个 Java 注解，用于标记一个方法，**在 Spring 容器初始化 bean 之后立即执行该方法**。它被用作初始化回调方法，用于在 bean 的依赖注入完成后执行一些必要的初始化逻辑。

**当一个 bean 被创建并且其依赖关系已经注入完成后，Spring 容器会调用带有 `@PostConstruct` 注解的方法。这个方法可以用来执行任何初始化操作，例如初始化成员变量、建立数据库连接、加载配置等。**

使用 `@PostConstruct` 注解的方法必须满足以下条件：
- 方法不能有任何参数。
- 方法的访问修饰符可以是 public、protected 或者默认的包内可见性。
- 方法不能被 static、final 或者 abstract 修饰。
- 方法只能在单例的 bean 上使用。

示例代码如下所示：

```java
@Component
public class MyBean {

    private String name;

    @PostConstruct
    public void init() {
        // 执行初始化逻辑
        this.name = "Initialized name";
        // 其他初始化操作...
    }

    // 其他方法...
}
```

在上面的例子中，`init()` 方法被标记为 `@PostConstruct`，当 Spring 容器创建 `MyBean` 实例并完成依赖注入后，会自动调用 `init()` 方法。在 `init()` 方法中，可以执行任何需要在 bean 初始化后进行的操作。

总而言之，`@PostConstruct` 注解可以确保在 Spring bean 的依赖注入完成后立即执行指定的初始化方法，以便进行一些必要的初始化操作。



### @value

**@Value 注解允许你将配置文件中的值注入到 Spring 管理的组件中，可以灵活地注入到字段、构造函数、方法参数或者注解上。**



@Value 是一个注解，用于在 Spring 中注入配置属性的值。它可以用于将配置文件中的值注入到类的字段、构造函数、方法参数或者注解上。

使用 @Value 注解时，你可以指定一个表达式或者直接指定一个属性的值。这个注解可以用于任何 Spring 管理的组件，如 Bean、Controller、Service 等。

下面是一些使用 @Value 注解的示例：

1. 注入配置文件中的值到字段：

```java
@Component
public class MyComponent {
    @Value("${my.property}")
    private String myProperty;

    // 其他代码...
}
```

在上面的例子中，my.property 是在配置文件中定义的属性，使用 @Value 注解将其注入到 myProperty 字段中。

### @EnableDiscoveryClient

以启用nacos的服务发现功能

### @EnableFeignClients

在启动类上加注解@EnableFeignClients，才能让我们定义的FeignClient生效



### @Controller和@RestController的一些区别

`@Controller` 和 `@RestController` 在 Spring 中有一些区别，特别是在处理视图（View）的方面。

1. **@Controller：**
   - 使用 `@Controller` 注解的类通常用于处理 Web 请求，并返回视图（View）。
   - 当一个方法使用 `@Controller` 注解时，它的返回值通常会被解释为视图的名称，然后被视图解析器解析成实际的视图路径。
   - 你可以使用 `return "item/item";` 这样的语句，它将会返回名为 "item/item" 的视图。

   ```java
   @Controller
   public class ItemController {
       @GetMapping("/showItem")
       public String showItem() {
           return "item/item";
       }
   }
   ```

2. **@RestController：**
   - 使用 `@RestController` 注解的类主要用于创建 RESTful Web 服务，它的方法通常返回数据而不是视图。
   - 当一个方法使用 `@RestController` 注解时，它的返回值将被直接转换成 JSON 或其他格式，而不是被解释为视图名称。
   - 如果你在 `@RestController` 中使用 `return "item/item";`，Spring 将会尝试将该字符串直接转换为 JSON，这通常不是你想要的行为。

   ```java
   @RestController
   public class ItemRestController {
       @GetMapping("/showItem")
       public String showItem() {
           return "item/item";  // 这将被转换为 JSON 或其他格式，不是视图
       }
   }
   ```

如果你在 `@RestController` 中希望返回一个视图，你可以使用 `ModelAndView` 类来封装视图名称和模型数据：

```java
@RestController
public class ItemRestController {
    @GetMapping("/showItem")
    public ModelAndView showItem() {
        ModelAndView modelAndView = new ModelAndView("item/item");
        // 添加模型数据，如果需要的话
        return modelAndView;
    }
}
```

总的来说，`@Controller` 适用于处理 Web 请求并返回视图，而 `@RestController` 适用于创建 RESTful Web 服务并返回数据。



## thymeleaf渲染引擎的使用

### 服务端渲染和客服端渲染

为了优化客户端渲染，使用**后端渲染(服务端渲染)**

客服端渲染过程：

- 浏览器发起请求，加载html页面
- 在成功获取html页面;，并在浏览器中生成DOM的时候，如果遇到js，css标签，又会去下载所需的js，css脚本
- 获取到js脚本后，如果js脚本又需要访问后端数据，那么又会向后端发起请求

这种渲染生成页面的方式，我们称之为客户端渲染，稍微思考一下，我们就会发现，从效率的角度出发，客户端渲染可能存在问题：

- 客户端渲染至少发起http请求三次，第一次是请求HTML页面，第二次是请求页面里的js脚本（渲染dom和进行事件绑定，而且这个过程时同步阻塞的），第三次是通过js请求获取动态数据，所以首次加载一个页面可能是比较慢的，就有可能出现白屏现象
- **在高并发场景下，页面中的动态数据，可能对应着多次异步请求，比如商品详情页中，商品的图片列表url，商品的名称，商品的销售属性等等，这些数据可能变化很少，但只要加载一次页面就都会发送请求，给后端服务器带来比较大的压力**

```
发起请求：浏览器向服务器发送请求，请求一个HTML文档。
下载HTML文档：服务器将HTML文档作为响应返回给浏览器。
解析HTML：浏览器开始解析HTML文档，并构建DOM（文档对象模型）树，表示网页的结构。
加载CSS和JavaScript：浏览器解析HTML文档时，如果遇到CSS和JavaScript的引用，会开始下载这些资源。
执行JavaScript：浏览器执行已下载的JavaScript代码。在执行过程中，JavaScript可以通过DOM API修改DOM树、添加事件处理程序等。
请求数据：JavaScript代码可以通过AJAX或Fetch等方式向服务器请求数据，通常是使用API或后端接口。
数据处理和渲染：一旦数据返回，JavaScript代码会使用这些数据来生成或更新页面的内容。这可能涉及数据处理、模板渲染和DOM操作。
更新DOM：根据JavaScript代码生成的新内容，浏览器会更新DOM树。这可能包括插入、删除、修改DOM元素等操作。
呈现页面：浏览器根据更新后的DOM树和CSS样式进行布局和渲染，将页面呈现给用户。
监听事件：页面呈现完毕后，浏览器会监听用户的交互事件（如点击、滚动等），并触发相应的事件处理程序。
```

如何**优化上述问题**呢？采用后端渲染，其核心思想如下：

- 与客户端渲染页面不同，后端渲染指的是，在返回页面的时候就将动态数据直接写入到返回给客户端的HTML文件中
- 这样一来，**客户端就不用在获取HTML页面后，再次通过异步请求来获取这些动态数据，而是直接使用包含所需数据的HTML，对于前端，少了请求数据的延迟，对于后端减少了处理请求的数量**
- 对于另外的一些需要根据用户的行为，来实时获取的动态数据，前端仍然可以使用ajax向后端发起动态请求实时获取



###  thymeleaf项目中的使用

项目中是在**web服务**（这个服务就是渲染页面的）中，通过openFegin从其他服务中获得数据， 然后再将这些数据动态渲染到html页面中

web服务 在nacos中的配置文件：

```yaml
spring:
  thymeleaf:
    mode: HTML5
    #编码 可不用配置
    encoding: UTF-8
    #开发环境配置为false,避免修改模板还要重启服务器， ★但在生产环境中，建议启用缓存以提高性能。
    cache: false
    #配置模板路径，默认是templates，可以不用配置
    prefix: classpath:/templates/
```







## 分布式锁 （缓存击穿）       [商品详情优化](D:\Java\java50th\java50-course-materials\04-微服务\01-课件\13_商品详情页2\商品详情优化.md)

### 基于setnx来实现    保证服务实例进来后用的都是同一把锁

```
# set not exists,设置一个键值对，不会覆盖原来的值
# 当key不存在的时候，再去设值，不会覆盖原来的值
setnx key value
```

要实现一把锁，最主要是要模拟一把锁加锁和释放锁的状态。我们可以在Redis中定义一个string类型的值，把这个值的key当做是锁的名字，于是我们可以用是否有该key对应的值，当做是锁是否上锁的状态：

- key对应的value值存在，说明锁被上锁了，不能重复加锁
- key对应的value值不存在，说明锁还没有被上锁，可以加锁(就是在redis中添加该key对应的value值)

![](D:/Java/java50th/java50-course-materials/04-微服务/01-课件/13_商品详情页2/商品详情优化.assets/商品详情页-setnx 加锁.png)

所以这样的加锁操作，刚刚好可以用SETNX来完成，所以可以改造我们的代码如下：

```java
@Service
public class TestServiceImpl implements TestService {

    @Autowired
    RedissonClient redissonClient;

    @Override
    public void incrWithLock() {
        // 在操作Redis中的数据之前先加锁, lock:number 对应的值可以是任意的
        RBucket<String> lockBucket = redissonClient.getBucket("lock:number");
        // trySet方法等价于SETNX
        boolean exists = lockBucket.trySet("lockObj");
        if (!exists) {
            // 如果锁已存在，即已经加锁, 则稍后重试
            try {
                Thread.sleep(100);
                incrWithLock();
                return;
            } catch (InterruptedException e) {
                e.printStackTrace();
            }

        }

        // 如果加锁成功，则自增key number对应的值
        try {
            RBucket<Integer> bucket = redissonClient.getBucket("number");
            // 获取key为number的value值
            int number = bucket.get();
            // 自增1
            number++;
            // 在放回redis
            bucket.set(number);
        } finally {
            // 访问完数据之后，释放锁，即删除lock:number这个key
            lockBucket.delete();
        }

    }
}

```

![](D:/Java/java50th/java50-course-materials/04-微服务/01-课件/13_商品详情页2/商品详情优化.assets/商品详情页-使用setnx结果.png)

看起来，好像也没啥问题，**但是这种实现方式有一个潜在的问题，就是如果在某一个商品服务实例中，加锁成功之后，因为某些原因，在还未释放锁之前，该实例挂了(java进程挂了)，那就意味着这把锁永远不会被释放，那么其他服务实例就再也访问不到这把锁了**。

### 增加过期时间     防止进程挂掉，永远不释放锁，这样其他实例，依然可以加锁成功

针对上述的可能存在的问题，我们可以增加一个解决方案就是，在利用SETNX加锁成功之后，给锁(给key)设置过期时间，这样一来，如果因为意外情况没有释放锁，到了锁的过期时间，其他服务实例，依然可以加锁成功。

![](D:/Java/java50th/java50-course-materials/04-微服务/01-课件/13_商品详情页2/商品详情优化.assets/setnx 增加过期时间.png)

所以，结合过期时间，我们改造代码如下：

```java
@Service
public class TestServiceImpl implements TestService {

    @Autowired
    RedissonClient redissonClient;

    @Override
    public void incrWithLock() {
        // 在操作Redis中的数据之前先加锁, lock:number 对应的值可以是任意的
        RBucket<String> lockBucket = redissonClient.getBucket("lock:number");
        // trySet方法等价于SETNX
        boolean exists = lockBucket.trySet("lockObj", 3, TimeUnit.SECONDS);
        if (!exists) {
            // 如果锁已存在，即已经加锁, 则稍后重试
            try {
                Thread.sleep(100);
                incrWithLock();
                return;
            } catch (InterruptedException e) {
                e.printStackTrace();
            }

        }

        // 如果加锁成功
        try {
            RBucket<Integer> bucket = redissonClient.getBucket("number");
            // 获取key为number的value值
            int number = bucket.get();
            // 自增1
            number++;
            // 在放回redis
            bucket.set(number);
        } finally {
            // 访问完数据之后，释放锁，即删除lock:number这个key
            lockBucket.delete();
        }

    }
}
```

以上代码如果测试是没问题的。但是这种实现方式，仍然有**潜在**的**问题**：

- **假设商品服务实例1先加锁成功，开始执行了，但是它执行4秒中，才会释放锁**
- **但是过了3秒后，锁过期了，商品服务实例2加锁成功**
- **又过了1s商品服务实例1执行完，释放锁，但是服务实例2还在执行，此时相当于没加锁**

### 增加UUID防止误删   实例1还未执行完时，锁已过期，实例2加锁后，被实例1误删掉锁

所以，为了防止锁被误删，所以在加锁的时候，我们给锁key对应的value，设置为一个uuid，并保存这个uuid。在释放锁的时候，如果获取到了锁，还要看看锁的value

- 如果锁key对应的value和释放锁的线程锁持有的uuid是不是同一个，说明是加锁线程在释放锁没有问题
- 但是如果不一致，说明加锁线程和释放锁的线程不是同一个，不能释放锁

![](D:/Java/java50th/java50-course-materials/04-微服务/01-课件/13_商品详情页2/商品详情优化.assets/商品详情页 增加uuid.png)

这样一来，就可以解决，锁的误删问题，代码如下

```java
@Service
public class TestServiceImpl implements TestService {

    @Autowired
    RedissonClient redissonClient;

    @Override
    public void incrWithLock() {
        // 在操作Redis中的数据之前先加锁, lock:number 对应的值可以是任意的
        RBucket<String> lockBucket = redissonClient.getBucket("lock:number");
        String uuid = UUID.randomUUID().toString();
        // trySet方法等价于SETNX，设置锁key对应的值
        boolean exists = lockBucket.trySet(uuid, 3, TimeUnit.SECONDS);
        if (!exists) {
            // 如果锁已存在，即已经加锁, 则稍后重试
            try {
                Thread.sleep(100);
                incrWithLock();
                return;
            } catch (InterruptedException e) {
                e.printStackTrace();
            }

        }

        // 如果加锁成功
        try {
            RBucket<Integer> bucket = redissonClient.getBucket("number");
            // 获取key为number的value值
            int number = bucket.get();
            // 自增1
            number++;
            // 在放回redis
            bucket.set(number);
        } finally {
            if (uuid.equals(lockBucket.get())) {
                // 说明是加锁线程在释放锁，可以正确释放
                lockBucket.delete();
            }

        }

    }
}

```

但是增加uuid防止误删就完美了吗？当然不是，因为还是有可能会有问题：

- **假设商品服务实例1，先加锁，访问完Redis数据后，刚刚执行完uuid.equals(lockBucket.get())发现结果为true，准备释放锁了**
- **但是在释放锁之前，刚好锁也过期了，商品服务实例2继续加锁成功**
- **然后，商品服务实例1删除锁，测试商品服务实例2在访问Redis数据时相当于没有加锁**

**究其原因，就是因为判断和释放锁不是原子操作。**

### Redisson实现的分布式锁    以上还有问题，判断和释放锁不是原子操作，还可能误删

所以，我们还需要让判断和释放锁成为原子操作，怎么样让它们成为原子操作呢？用Lua脚本来实现它们即可。因为Redis的工作线程是单线程，且Lua脚本可以直接在Redis中运行，所以一段Lua脚本中运行的必然是一个原子操作。而Redisson底层，就是利用Lua脚本来加锁和释放锁的。

如果使用Redisson，则代码如下：

```java
@Service
public class TestServiceImpl implements TestService {

    @Autowired
    RedissonClient redissonClient;

    @Override
    public void incrWithLock() {
        // 获取锁
        RLock redisLock = redissonClient.getLock("lock:number");
        try {
            // 加锁，失败会在这里阻塞
            redisLock.lock();
            // 加锁成功，代码执行到这里
            RBucket<Integer> bucket = redissonClient.getBucket("number");
            // 获取key为number的value值
            int number = bucket.get();
            // 自增1
            number++;
            // 在放回redis
            bucket.set(number);
        } finally {
            // 释放锁
            redisLock.unlock();
        }

    }
}

```

为了确保**分布式锁可用**，我们要确保锁的实现同时满足以下**四个条件**：

**\- 互斥性。在任意时刻，只有一个客户端能持有锁。**

**\- 不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。**

**\- 解铃还须系铃人。加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了。**

**\- 加锁和解锁必须具有原子性**

**而这四个条件，Redisson实现的分布式锁都可以满足，同时Redisson实现的分布式锁，还是可重入的。**



## 自定义注解

### 语法：

```
权限修饰符 @interface 注解名字{
    // 注解体定义
    属性类型 属性名();
    属性类型 属性名();
    属性类型 属性名();
    ......
}

例如：

// 定义注解
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
@interface Login{
    // 属性
    String username() default "admin";

    String password() default "123456";
}
```

### 元注解

**@Retention元注解，来定义我们自己定义的注解的保留级别.**

- RetentionPolicy.RUNTIME
- RetentionPolicy.CLASS  默认
- RetentionPolicy.SOURCE

**@Target元注解，注解可以作用的目标**

对于注解而言，可以作用的目标：

1. 整个类    ElementType.TYPE
2. 成员变量   ElementType.FIELD
3. 构造方法   ElementType.CONSTRUCTOR
4. 成员方法   ElementType.METHOD

### 注意事项

**使用的时候注解的每个属性都要赋值**

```java
@注解名(属性1=属性值,属性2=属性值)
```

**注意事项:**

- 每个属性都要赋值
- 可以不赋值,但是要有默认值, default
- 数组形式赋值 {}
- 如果只有1个属性, 名字叫value, 可以简化赋值
- 如果属性类型是引用类型, 不能是null

### 注解的属性值

自定义注解的属性可以用于在注解中存储和**传递信息**，以便在需要时能够根据这些信息进行相应的处理。通过注解的属性，你可以在注解中提供一些可配置的选项，使得注解能够适应不同的使用场景。

## AspectJ

​	切面组件

- 增加AspectJ的注解开关 → 配置类上增加一个注解**@EnableAspectJAutoProxy**
- 把组件标记为切面组件 → **@Aspect**

### 切入点表达式

引入Aspectjweaver依赖

```xml
<dependency>
    <groupId>org.aspectj</groupId>
    <artifactId>aspectjweaver</artifactId>
    <version>1.9.6</version>
</dependency>
```

在切面组件中使用**@Pointcut**

- value属性：切入点表达式
- 方法名：作为切入点id

```java
 @Retention(RetentionPolicy.RUNTIME)
@Target({ElementType.METHOD})
public @interface Pointcut {
    String value() default "";

    String argNames() default "";
}
```

###  execution

- 能否省略

- 能否通配

- 特殊用法

修饰符：可以省略，省略代表任意修饰符

返回值：不能省略，POJO类要写全类名；可以是用*作为通配符

包名、类名、方法名：使用..部分省略，头和尾不能省略，中间的任意一部分都可以省略；可以使用*作为通配符

形参：可以使用..或* 来进行通配，<font color=red>..代表任意数量任意类型的参数，*代表任意类型的参数；POJO类要写全类名</font>

```java
<!--execution 👉 aop:pointcut 👉 指定的是容器中的组件中的方法-->

<!--语法：execution(访问修饰符 返回值 包名.类名.方法名(参数列表))

        1、能否省略不写
        2、能否通配
        3、是否有特殊用法

        访问修饰符：可以省略、省略不写代表任意修饰符
        返回值：不可以省略、可以使用*来通配、JavaBean要写全类名（基本类型、包装类以及String可以直接写）
        包名、类名、方法名：可以部分省略，头和尾不能省略，中间的任意一部分都可以省略 👉 ..来进行省略
                         *来通配 com.*、com.cskao*，头和尾也可以使用*作为通配符                                          *.cskaoyan.service.UserService.say*
        参数列表：可以省略 👉 代表的是无参的方法
                如果想要写多个参数，写参数的全类名 👉 第一参数是String，第二个参数是int 👉 (String,int)
                可以通配：* 👉 代表的单个任意参数
                        .. 👉 任意参数 👉 任意数量的任意类型的参数
                JavaBean要写全类名（基本类型、包装类以及String可以直接写）
-->
/**
     * execution(修饰符 返回值 包名、类名、方法名(形参))
     * 可以增强多个方法
     *     越具体，匹配范围越小；越宽泛，匹配范围越大。
     *     通配符
     *     - 修饰符： 可以省略不写，如果省略不写代表任意修饰符
     *     - 返回值： 不能省略，可以使用通配符* 。*代表任意值
     *               如果是引用类型，要写全限定类名;全限定类名中也可以出现通配符*
     *     - 包名、类名、方法名： 可以使用*来通配，也可以使用..来通配
     *                         头和尾的位置不能使用..  但是可以使用*
     *     - 形参： 省略不写代表无参方法
     *             可以使用*来通配 → 代表单个任意类型的参数
     *             也可以使用..来通配 → 代表任意参数 → 数量任意，类型也任意
     *             如果是引用类型，要写全限定类名;全限定类名中也可以出现通配符*
     */
    //@Pointcut("execution(public void com.cskaoyan.demo3.service.UserServiceImpl.sayHello(java.lang.String))")
    //@Pointcut("execution(public * com.cskaoyan.demo3.service.UserServiceImpl.sayHello(java.lang.String))")
    //@Pointcut("execution(public com.cskaoyan.demo3.bean.User com.cskaoyan.demo3.service.UserServiceImpl.query(Integer))")
    //@Pointcut("execution(public com.cskaoyan.demo3.bean.* com.cskaoyan.demo3.service.UserServiceImpl.query(Integer))")
    //@Pointcut("execution(public com.cskaoyan.demo3.bean.* com.cskaoyan.*.ser*.*ServiceImpl.query(Integer))")
    //@Pointcut("execution(public com.cskaoyan.demo3.bean.* com..*ServiceImpl.query(Integer))")
    // 增强service层 以say作为开头的方法(暂时先没管参数)
    //@Pointcut("execution(* com..service.*ServiceImpl.say*(String))")
    @Pointcut("execution(* com..service.*ServiceImpl.*(..))")
    public void pointcut1(){}
```

```java
@Pointcut("execution(public void com.cskaoyan.service.UserServiceImpl.sayHello(String))")
public void mypointcut1(){}
```

**execution和@annotation有什么区别：**

`execution`和`@annotation`是用于定义切点表达式（pointcut expression）的两种不同方式。

- `execution`是一种切点表达式，它基于方法的签名和访问修饰符来匹配方法。它可以定义在切面中使用的切点，以便选择特定的方法进行增强或拦截。例如，`execution(public void com.example.MyClass.myMethod())`表示选择`MyClass`类中的`myMethod`方法，而`public`是访问修饰符。
  
- `@annotation`是一种切点指示器（pointcut designator），它基于注解的存在来匹配方法。它可以用于切面中的切点表达式，以便选择被特定注解标记的方法进行增强或拦截。例如，`@annotation(com.example.MyAnnotation)`表示选择被`MyAnnotation`注解标记的方法。

区别在于：
- `execution`是基于方法的签名和访问修饰符进行匹配，而`@annotation`是基于注解的存在进行匹配。
- `execution`可以匹配方法的任何特征（如方法名、参数类型等），而`@annotation`只匹配被指定注解标记的方法。
- `execution`可以选择多个方法进行匹配，而`@annotation`只能选择被特定注解标记的方法。

在使用切面编程时，你可以根据需要选择使用`execution`还是`@annotation`，**具体取决于你想要匹配哪些方法和条件**。

### @annotation   (只是一些不同的匹配方式)

写上自定义注解的全类名

```java
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
public @interface CountTime {
}
```

```java
@Pointcut("@annotation(com.cskaoyan.CountTime)")
public void mypointcut2(){}
```

### @target

写上自定义注解的全类名

```java
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.Type)
public @interface CountTimeAllMethod {
}
```

```java
@Pointcut("@target(com.cskaoyan.CountTimeAllMethod)")
public void mypointcut3(){}
```

### Aspect切面

在切面类中配置切面组件和通知方法

AspectJ的注解实现，首先要标记前面开关

然后在切面类中使用AspectJ相关的注解，表达切面、切入点、通知

```java
/**
 * 增加对应通知的方法 ： 将这样的一些方法配置为对应的通知方法
 * 方法 👉 通知
 *  before
 *  after
 *  around
 *  afterReturning
 *  afterThrowing
 */
@Component
@Aspect
public class CustomAspect {
  /**
     * pointcut方法
     * 返回值：void
     * 方法名：任意写 👉 用于作为pointcut组件的id
     * 形参：不用写
     * 方法体：不用写
     * @Pointcut的value属性写切入点表达式
     */
  @Pointcut("execution(* com.cskaoyan.service..*(..))")
  public void mypointcut1(){}
  @Pointcut("@annotation(com.cskaoyan.CountTime)")
  public void mypointcut2(){}
  @Pointcut("@annotation(com.cskaoyan.CountTimeAllMethod)")
  public void mypointcut3(){}
  /**
     * 通知注解@Before、@After、@Around、@AfterReturning、@AfterThrowing
     * value属性：
     *      1、对应的切入点表达式
     *      2、引用的切入点表达式的方法名
     */

  /**
     * before通知
     * 返回值：void
     * 方法名：任意去写
     * 形参：joinPoint连接点 (可写可不写)
     */
  //@Before("execution(* com.cskaoyan.service..*(..))")
  @Before("mypointcut()")
  public void before(JoinPoint joinPoint){
    System.out.println("before");
  }
  /**
     * after通知
     * 返回值：void
     * 方法名：任意去写
     * 形参：joinPoint连接点 (可写可不写)
     */
  @After("mypointcut()")
  public void after(){
    System.out.println("after");
  }
  /**
     * around通知
     * 返回值：Object
     * 方法名：任意去写
     * 形参：ProceedingJoinPoint连接点 (必须写) 👉 提供了proceed方法，执行的是委托类的代码
     */
  @Around("mypointcut()")
  public Object around(ProceedingJoinPoint joinPoint) throws Throwable {
    System.out.println("around的before");

    Object proceed = proceed = joinPoint.proceed();//该方法执行的是委托类的方法

    System.out.println("around的after");
    return proceed;
  }
  /*public Object around(ProceedingJoinPoint joinPoint){
        System.out.println("around的before");
        Object proceed = null;//该方法执行的是委托类的方法
        try {
            proceed = joinPoint.proceed();
        } catch (Throwable throwable) {
            throwable.printStackTrace();
        }
        System.out.println("around的after");
        return proceed;
    }*/
  /**
     * afterReturning通知
     * 返回值：void
     * 方法名：任意去写
     * 形参：Object(委托类方法的执行结果)
     */
  @AfterReturning(value = "mypointcut()",returning = "result")
  public void afterReturning(Object result){
    System.out.println("afterReturning");
  }
  /**
     * afterThrowing通知
     * 返回值：void
     * 方法名：任意去写
     * 形参：Exception/Throwable(委托类方法执行过程中抛出异常)
     */
  @AfterThrowing(value = "mypointcut()",throwing = "exception")
  public void afterThrowing(Exception exception){//Throwable
    System.out.println("afterThrowing");
    System.out.println(exception.getMessage());
  }

}
```

### JoinPoint连接点

获取增强过程中的一些值

- Signature 方法
- Arguments 参数
- This 代理对象
- Target 委托类对象

在通知方法的形参中，传入JoinPoint形参，通过JoinPoint获得对应的一些参数

```java
@Before("mypointcut()")
public void before(JoinPoint joinPoint){
    //Signature 方法的描述
    //This 代理对象
    //Target 委托类对象
    //Arguments 参数
    Signature signature = joinPoint.getSignature();
    Object proxy = joinPoint.getThis();
    Object target = joinPoint.getTarget();
    Object[] args = joinPoint.getArgs();

    System.out.println("signature:" + signature.getName());
    System.out.println(proxy.getClass().getName());
    System.out.println(target.getClass().getName());
    System.out.println(Arrays.asList(args));
}
```

## AOP+自定义注解+分布式锁   [商品详情优化](D:\Java\java50th\java50-course-materials\04-微服务\01-课件\13_商品详情页2\商品详情优化.md)

通过自定义注解+aop将与业务逻辑无关的功能模块化，提高代码的可维护性和可重用性。

整个流程的逻辑：在需要被增强的方法（**比如getSkuInfo()方法等**）上面添加自定义注解如@RedisCache， 然后再单独写一个切面，通知方法使用环绕通知 @Around("**@annotation**(com.cskaoyan.mall.common.cache.**RedisCache**)")，其中@annotation匹配自定义注解@RedisCache；那么在所有有@RedisCache注解的方法就会执行增强方法：

**1. 先访问Redis，有数据直接放回，没有数据则加锁；**  **（around通知）**    **通过 getAnnotation(RedisCache.class) 可以获取到当前方法上标注的 RedisCache 注解对象。（通过获取这个注解对象的前缀，最终可以去访问redis中的数据，如果有就返回，没有就加锁访问数据库）**

**2. 然后执行自己本身的方法，从数据库中拿数据；**  **（自身方法）**

**3.因为是around通知， 最后再执行增强逻辑， 将拿到的数据，放入Redis中，并释放锁。**  **（around通知）**

=================================================================================================================================================================================================================================================================================================================

我们在前面说过，**商品详情页时访问量比较大的，所以我们需要给整个商品详情页的数据增加缓存(除了商品价格这种实时数据)。因此我们不仅仅需要对获取SKU基本信息的getSkuInfo方法做改造，还需要对其他获取商品详情页数据的方法做改造（其他方法看下文代码）。**

但是，无论对于哪个方法做改造，改造的逻辑几乎都是一样的，即我们需要给多个方法做增强，增加一段通用处理逻辑，此时你会 想到什么呢？当然是AOP

既然要是用AOP，自然需要定义切面，但在定义切面之前，我们需要思考以下两个问题：

- 我们的切入点应该如何定义
- 我们应该使用什么样的通知类型

针对第一个问题，我们可以结合自定义注解，给**需要增强的方法加自定义注解**，所以我们的切入点使用@annotation注解找具有目标自定义注解的方法即可。

针对第二个问题，得根据我们的增强逻辑来决定，被增强的方法是访问数据库的，而我们的增强逻辑需要在访问数据库之前先访问缓存，如果缓存中没有，在被增强方法访问数据库之后，我们还需要将数据库中的查询结果，放入Redis，所以很显然，我们应该使用环绕通知。

![](D:\Java\java50th\java50-course-materials\04-微服务\01-课件\13_商品详情页2\商品详情优化.assets\商品详情页优化-通知.png)

![](D:\Java\java50th\java50-course-materials\04-微服务\01-课件\13_商品详情页2\商品详情优化.assets\aop cache.png)

所以我们可以定义自定义注解如下：

```java
@Target({ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
public @interface RedisCache {

    // 给缓存数据增加前缀，以区分不同的缓存数据
    String prefix() default "cache:";

}
```

定义切面如下：

```java
@Component
@Aspect
public class RedisCacheAspect {

    @Autowired
    private RedissonClient redissonClient;

    //  定义一个环绕通知！
    @Around("@annotation(com.cskaoyan.mall.common.cache.RedisCache)")
    public Object gmallCacheAspectMethod(ProceedingJoinPoint point) {
        //  定义一个对象
        Object obj = null;
        MethodSignature methodSignature = (MethodSignature) point.getSignature();
        RedisCache redisCache = methodSignature.getMethod().getAnnotation(RedisCache.class);
        //   获取到注解上的前缀
        String prefix = redisCache.prefix();
        //  组成缓存的key！ 获取方法传递的参数
        String key = prefix + Arrays.asList(point.getArgs()).toString();
        RLock lock = null;
        try {
            //  可以通过这个key 获取缓存的数据
            obj = this.redissonClient.getBucket(key).get();
            if (obj != null) {
                // 获取到了直接返回
                return obj;
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
        try {
            //  加锁
            lock = redissonClient.getLock(key + ":lock");
            lock.lock(RedisConst.SKULOCK_EXPIRE_PX2, TimeUnit.SECONDS);
            Object redisData = this.redissonClient.getBucket(key).get();
            // double check
            if (obj != null) {
                // 获取到了直接返回
                return obj;
            }

            //  执行业务逻辑：直接从数据库获取数据
            obj = point.proceed(point.getArgs());

              // 将结果放入redis
            obj = putInRedis(obj, key, methodSignature);

            return obj;

        } finally {
            //  解锁
            if (lock != null) {
                lock.unlock();
            }
        }
    }

   /*
         将数据放入Redis缓存
     */
    private Object putInRedis(Object obj, String key, MethodSignature methodSignature) {
        try {
            if (obj == null) {
                //  防止缓存穿透
                //obj = new Object();
               Class returnType = methodSignature.getReturnType();
                if (returnType.isAssignableFrom(List.class)) {
                    // 返回值是Collection或List类型
                    obj = new ArrayList();
                } else if (Map.class.equals(returnType)) {
                    // 返回值是Map类型
                    obj = new HashMap();
                } else {
                    // 其他类型
                    Constructor declaredConstructor = returnType.getDeclaredConstructor();
                    declaredConstructor.setAccessible(true);
                    obj = declaredConstructor.newInstance();
                }
                //  将缓存的数据变为 Json 的 字符串,默认值的过期时间是1分钟
                this.redissonClient.getBucket(key).set(obj, RedisConst.SKUKEY_TEMPORARY_TIMEOUT, TimeUnit.SECONDS);
            } else {
                //  将缓存的数据变为 Json 的 字符串
                this.redissonClient.getBucket(key).set(obj, RedisConst.SKUKEY_TIMEOUT, TimeUnit.SECONDS);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }

        return obj;
    }

}

```

定义好切面后，我们可以在获取详情页数据的时候使用了

```java
    @RedisCache(prefix = RedisConst.SKUKEY_PREFIX)
    @Override
    public SkuInfoDTO getSkuInfo(Long skuId) {
        ...
    }

    @RedisCache(prefix = "spuSaleAttrListCheckBySku:")
    @Override
    public List<SpuSaleAttributeInfoDTO> getSpuSaleAttrListCheckBySku(Long skuId, Long spuId) {
	   ...
    }

    @Override
    @RedisCache(prefix = "skuValueIdsMap:")
    public Map<String, Long> getSkuValueIdsMap(Long spuId) {
        ...
    }

    @Override
    @RedisCache(prefix = "categoryHierarchyByCategory3Id:")
    public CategoryHierarchyDTO getCategoryViewByCategoryId(Long category3Id) {
	   ...
    }
    
    @Override
    @RedisCache(prefix = "SpuPosterList:")
    public List<SpuPosterDTO> findSpuPosterBySpuId(Long spuId) {
	   ...
    }

    @RedisCache(prefix = "platformAttributeInfoList:")
    @Override
    public List<PlatformAttributeInfoDTO> getPlatformAttrInfoBySku(Long skuId) {
		...
    }
```

## 布隆过滤器    解决缓存穿透问题 

**项目中是在商品服务的config包里，去实现CommandLineRunner接口即可**

**在商品上架的时候（onSale（）方法），向布隆过滤器中添加元素；然后在获取商品详情时判断，该元素是否在布隆过滤器中。**

方法二：空对象缓存（若使用空对象缓存会暂用一定的空间）

如果我们想要判断，一个skuId是否存在于数据库，我们只需要在构造布隆过滤器时，将目标集合变为数据库中所有的SKU商品的id集合即可。

**Redisson本身对实现了基于Redis的布隆过滤器，可以非常方便的使用**

```java
       //  根据指定名称(key)获取布隆过滤器
	   RBloomFilter rbloomFilter = redissonClient.getBloomFilter("xxx");
        // 初始化布隆过滤器，预计统计元素数量为100000，期望误判率为0.01
       rbloomFilter.tryInit(100000, 0.01);
        
         
       // 向布隆过滤器中添加目标元素
       rbloomFilter.add(目标元素);

       // 判断目标元素是否存在，返回false表示不存在
       boolen exists = rbloomFilter.contains(目标元素);
```

★★★ 在我们的商品服务中，**我们需要在服务启动的时候就执行对于布隆过滤器的初始化(即仅仅需要执行一个操作)**，所以我们可以将布隆过滤器的初始化，可以使用**CommandLineRunner**接口

```java
public interface CommandLineRunner {

	/**
	   ★★★该方法会被SpringBoot在初始化完Spring容器之后自动调用
	 * Callback used to run the bean.
	 * @param args incoming main method arguments
	 * @throws Exception on error
	 */
	void run(String... args) throws Exception;

}
```

所以我们可以这样使用

```java
/*
    这个BloomFilterRunner因为加了Component注解，所以会被放到Spring容器中，
    Spring容器初始化完毕后，该对象的run方法会被自动调用
*/
@Component
public class BloomFilterRunner implements CommandLineRunner {
    
    @Autowired
    RedissonClient redissonClient;
    
    @Override
    public void run(String... args) throws Exception {
        RBloomFilter<Long> rbloomFilter = redissonClient.getBloomFilter(RedisConst.SKU_BLOOM_FILTER);
        // 初始化布隆过滤器，预计统计元素数量为100000，期望误差率为0.01
        rbloomFilter.tryInit(100000, 0.01);
    }
}
```

然后我们在上架SKU商品的时候，向布隆过滤器中添加元素

```java
    @Override
    public void onSale(Long skuId) {
		
        /* 
           ...
        */
        
        //向添加布隆过滤器添加元素
        RBloomFilter<Long> rbloomFilter = redissonClient.getBloomFilter(RedisConst.SKU_BLOOM_FILTER);
        rbloomFilter.add(skuId);

    }
```

在处理获取详情页商品信息的请求时，首先判断布隆过滤器中是否存在该元素，如果不存在，则直接返回默认值

```java
@Override
    public ProductDetailDTO getItemBySkuId(Long skuId) {

        ProductDetailDTO productDetailDTO = new ProductDetailDTO();
        RBloomFilter<Long> bloomFilter = redissonClient.getBloomFilter(RedisConst.SKU_BLOOM_FILTER);
        if (!bloomFilter.contains(skuId)) {
            // 如果不存在，则返回默认值
            return productDetailDTO;
        }

		/*
		  ....
		*/
        return productDetailDTO;
    }
```

## 倒排索引和正排索引

**倒排索引：“关键词” →“文档Id”，即关键词到文档id的映射。**



正排索引和倒排索引在不同的场景下具有不同的优势，无法简单地说哪个更出色，而取决于具体的需求和使用场景。

**正排索引（Forward Index）的优势：**

- 正排索引适用于范围查询和有序查询，因为数据按照索引顺序进行排序。
- 正排索引在查询时可以直接定位到对应的数据行，因为叶子节点包含了数据。
- 正排索引对于频繁的等值查询非常高效。

**倒排索引（Inverted Index）的优势：**

- 倒排索引适用于关键词搜索和全文搜索，因为它可以快速找到包含特定关键词的文档。
- 倒排索引可以支持高效的词频统计和相关性排序。
- 倒排索引适用于大规模文本检索和搜索引擎等应用。

在实际应用中，可以根据具体的需求和场景选择适合的索引类型。有些情况下可能需要同时使用正排索引和倒排索引，以充分利用它们的优势，提高查询效率和准确性。



## OpenFeign

### 基本使用

**openfeign只在服务的消费端使用，对于服务的提供端来说是无感的，只需要关注自身的逻辑业务即可。**

**只需要在需要在服务消费端写一个xxxClient接口，添上@FeignClient注解，接口里面是想要使用的方法（要一模一样），然后再在启动类上添加@EnableFeignClients注解即可**



在使用@FeignClient注解时，value属性用于指定要调用的目标服务的名称。**该名称通常是服务注册中心中注册的服务名**。

```java
// 注意，这里FeignClient的名字是调用的服务的名称 / 服务中心注册的服务名
@FeignClient("feign-provider-8005")
public interface DemoServiceClient{
    @GetMapping("/feign/hello")
    String sayHello(@RequestParam(name = "name")String name);
}
```

在启动类上加注解@EnableFeignClients，才能让我们定义的FeignClient生效

```java
@SpringBootApplication
@EnableDiscoveryClient
@EnableFeignClients
public class FeignConsumerApplication {
    public static void main(String[] args) {
        SpringApplication.run(FeignConsumerApplication.class, args);
    }
}
```

### FeignInterceptor拦截器   通过拦截器在服务与服务之间传递用户信息



1. 网关路由分发到web-all时有用户的信息（网关通过cookie中的token去获取）；
2. web-all远程调用order服务时没有用户的信息（因为这是另一个请求） ；
3. 通过FeignInterceptor（会在远程调用发起请求前生效，也就是web-all在远程调用前生效），从web-all阶段获取用户的信息，然后把信息添加到请求头中；这样web-all调用order服务的请求头里有就有用户的信息了。

### 项目中OpenFeign的具体使用

1. 在商品服务中，用到了搜索服务的incrHotScore()方法
2. 搜素服务中，调用了商品服务的getSkuInfo()方法、getCategoryView()方法、getTradeMark()等方法



## Elastic Search       [Elastic Search2](D:\Java\java50th\java50-course-materials\04-微服务\01-课件\15_elasticsearch-2\Elastic Search2.md)  ←Java api   [掘金]([ES使用和搜索中心的建设（一:基础介绍与语法） - 掘金 (juejin.cn)](https://juejin.cn/post/6844904116783677454?searchId=202312211033555F381008EC9A77C100BE))

![](D:\Desktop\calligraphy-pic\es.png)



### match all

match all 相当于不加查询条件搜索所有的文档 

```json
GET product/_search
{
  "query": {
    "match_all": {}
  },
  "from": 0,
  "size": 100
}
```

### term查询

erm查询和字段类型有关系，首先回顾一下ElasticSearch两个数据类型

 ElasticSearch两个数据类型：

- text：会分词，不支持聚合
- keyword：不会分词，将全部内容作为一个词条，支持聚合

**term查询：不会对查询条件进行分词。但是注意，term查询，查询text类型字段时，文档中类型为text类型的字段本身仍然会分词**

```json
GET product/_search
{
  "query": {
    "term": {
      "title": {
        "value": "手机充电器"
      }
    }
  }
}
```

### match查询

match查询的特征：

•会对查询条件进行分词。

•然后将分词后的查询条件和目标字段分词后的词条进行等值匹配

•默认取并集（OR），即只要查询条件中的一个分词和目标字段值的一个分词(词条)匹配，即认为匹配查询条件

```json
# match查询
GET product/_search
{
  "query": {
    "match": {
      "title": "手机充电器"
    }
  },
  "size": 500
}
```

**match 的默认搜索（or 并集）**例如：华为手机，会分词为 “华为”，“手机” 只要出现其中一个词条都会认为词条匹配

**match的 and（交集） 搜索**，例如：例如：华为手机，会分词为 “华为”，“手机”  但要求“华为”，和“手机”同时出现在词条中，才算词条匹配

```json
GET product/_search
{
  "query": {
    "match": {
      "title": {
        "query": "华为手机",
        "operator": "and"    #如果是or，那么"小米"或 "手机"任何一个匹配，都会匹配成功 
      }
    }
  },
  "size": 500
}
```

###  模糊查询

####  wildcard查询

wildcard查询: wildcard查询：会对查询条件进行分词。还可以使用通配符 ?（任意单个字符） 和  * （0个或多个字符）

```json
# wildcard 查询。查询条件分词，模糊查询
GET product/_search
{
  "query": {
    "wildcard": {
      "title": {
        "value": "手机*"
      }
    }
  }
}
```

#### 正则查询

```
[A-Z a-z 0-9_] 表示一个大小写英文字符，或者0-9的数字字符，或者下划线字符_

+号多次出现

(.)*为任意字符
正则查询取决于正则表达式的效率
```

```json
GET product/_search
{
  "query": {
    "regexp": {
      "title": "[A-Z a-z 0-9_]+(.)*"
    }
  }
}
```

#### 前缀查询

 对keyword类型支持比较好

```json
# 前缀查询 对keyword类型支持比较好
GET product/_search
{
  "query": {
    "prefix": {
      "brandName": {
        "value": "三"
      }
    }
  }
}
```

####  模糊查询Java API

```java
//模糊查询
WildcardQueryBuilder wildQuery = QueryBuilders.wildcardQuery("title", "充电*");//充电后多个字符
//正则查询
 RegexpQueryBuilder regexQuery = QueryBuilders.regexpQuery("title", "[A-Z a-z 0-9_]+(.)*");
 //前缀查询
 PrefixQueryBuilder prefixQuery = QueryBuilders.prefixQuery("title", "充电");
```

### querystring

queryString 多条件查询

1. 会对查询条件进行分词。
2. 然后将分词后的查询条件和词条进行等值匹配
3. 默认取并集（OR）
4. 可以指定多个查询字段

**query_string：可以识别query中的连接符（or 、and）**

```json
# queryString

GET product/_search
{
  "query": {
    "query_string": {
      "fields": ["title","sell_point"], 
      "query": "耳机 AND 充电器"
    }
  }
}
```

java代码

```java
QueryStringQueryBuilder query = QueryBuilders.queryStringQuery("耳机充电器").field("title").field("sellPoint");
```

**simple_query_string：不识别query中的连接符（or 、and）**，查询时会将 “耳机”、"and"、“充电器”分别进行查询

```json
GET product/_search
{
  "query": {
    "simple_query_string": {
      "fields": ["title","sell_point"], 
      "query": "耳机 AND 充电器"
    }
  }
}
```

java代码

```java
QueryStringQueryBuilder query = QueryBuilders.queryStringQuery("耳机充电器").field("title").field("sellPoint")
```



### 范围& 排序查询

```json
GET product/_search
{
  "query": {
    "range": {
      "price": {
        "gte": 100,
        "lte": 1000
      }
    }
  },
  "sort": [
    {
      "price": {
        "order": "desc"
      }
    }
  ]
}	
```

```java
 //范围查询 以price 价格为条件
RangeQueryBuilder query = QueryBuilders.rangeQuery("price");

//指定下限
query.gte(100);
//指定上限
query.lte(1000);

sourceBuilder.query(query);

//排序  价格 降序排列
sourceBuilder.sort("price",SortOrder.DESC);
```

### 复合查询 bool

 boolQuery：对多个查询条件连接。其组成主要分为如下四个部分：

1. must（and）：条件必须成立
2. must_not（not）：条件必须不成立
3. should（or）：条件可以成立
4. filter：条件必须成立，性能比must高。不会计算得分

```json
# must
GET product/_search
{
  "query": {
    "bool": {
      "must": [
        {
           "term": {
             "title": {
               "value": "充电器"
             }
           }
        },
        {
          "match": {
            "sellPoint": "快充"
          }
        }
      ]
    }
  }
}

```



```json
# must_not
GET product/_search
{
  "query": {
    "bool": {
      "must_not": [
        {
           "match": {
             "title": "充电器"
           }
        }
      ]
    }
  }
}
```



```json
# should 中的多个条件是or关系
GET product/_search
{
  "query": {
    "bool": {
      "should": [
          {
           "term": {
             "title": {
               "value": "充电器"
             }
           }
        },
        {
          "term": {
             "sellPoint": {
               "value": "小菜鸡"
             }
           }
        }
      ]
    }
  }
}
```



```json
# filter
GET product/_search
{
  "query": {
    "bool": {
      "filter": [
          {
           "term": {
             "title": {
               "value": "充电器"
             }
           }
        },
        {
          "match": {
            "sellPoint": "快充"
          }
        }
      ]
    }
  }
}
```

这里有几点需要注意：

- 一个复合查询中，可以同时包含must，must not，should，filter中的一个或多个部分
- 每一部分，都可以包含多个查询条件(只有should中的多个查询条件是or关系)
- 当存在must，或者filter的时候，should中的条件默认不生效
- must和filter都可以表示同时满足多个条件的查询，但是不同的地方在于must会计算文档的近似度得分，filter不会(must_not也不会)

```json
# boolquery 包含多个部分
GET product/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "title": {
              "value": "充电器"
            }
          }
        }
      ],
      "filter":[ 
        {
        "term": {
          "title": "原装"
        }
       },
       {
         "range":{
          "price": {
            "gte": 40,
            "lte": 100
         }
         }
       }
      
      ]
    }
  }
}

```

JAVA API:

布尔查询：boolQuery 

1. 查询商品为(title): 充电器 
2. 查询过滤条件：原装
3. 查询价格在：40-100 

```java
        //1.构建boolQuery
        BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();
        //2.构建各个查询条件
        //2.1 查询品牌名称为:华为
        TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery("title", "耳机");
        boolQuery.must(termQueryBuilder);
        //2.2. 查询标题包含：手机
        MatchQueryBuilder matchQuery = QueryBuilders.matchQuery("title", "原装");
        boolQuery.filter(matchQuery);

        //2.3 查询价格在：40-100
        RangeQueryBuilder rangeQuery = QueryBuilders.rangeQuery("price");
        rangeQuery.gte(40);
        rangeQuery.lte(100);
        boolQuery.filter(rangeQuery);

        sourceBuilder.query(boolQuery);
```

### 聚合查询

聚合查询分为两种类型:

- 指标聚合：相当于MySQL的聚合函数。max、min、avg、sum等
- 桶聚合：相当于MySQL的 group by 操作。不要对text类型的数据进行分组，会失败。

```json
# 聚合查询

# 指标聚合 聚合函数

GET product/_search
{
  "query": {
    "match": {
      "title": "耳机"
    }
  },
  "aggs": {
    "max_price": {
      "max": {
        "field": "price"
      }
    }
  }
}

# 桶聚合  分组
GET product/_search
{
  "query": {
    "match": {
      "title": "充电器"
    }
  },
  "aggs": {
    "price_bucket": {
      "terms": {
        "field": "price",
        "size": 100
      }
    }
  }
}
```

### 高亮查询

高亮三要素：

1. 高亮字段
2. 前缀
3. 后缀

默认前后缀 ：em

```html
<em>手机</em>
```

```json
GET product/_search
{
  "query": {
    "match": {
      "title": "充电器"
    }
  },
  "highlight": {
    "fields": {
      "title": {
        "pre_tags": "<font color='red'>",
        "post_tags": "</font>"
      }
    }
  }
}
```

## Elastic Search       Java中的使用

### 总结 （共四步如下）

**1. 先引入依赖spring-boot-starter-data-elasticsearch ， 然后在配置文件中添加配置；2. 然后定义一个实体类去映射ES索引中的文档；3 . 然后定义一个接口去继承ElasticsearchRepository，注入其对象后就可以快速进行crud（有内置的方法更简便）、分页、排序等简单操作 （上架、下架、更新热点）；4.引入ElasticsearchRestTemplate用来进行更复杂的自定义查询（crud要自己构造更复杂），如构造自定义分页，高亮，nested以及聚合查询等 **



**补充★**：上架的时候会远程调用商品服务，拿到信息，最后封装成Goods类，因为定义的接口为以下：（映射的实体类是Goods）

```java
public interface GoodsRepository extends ElasticsearchRepository<Goods,Long> {

}
```





### 基础配置 

要使用Elasticsearch我们需要引入如下依赖： 

```xml
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-elasticsearch</artifactId>
            <version>2.1.7.RELEASE</version>
        </dependency>
```

还需要在配置文件中增加如下配置

```yml
spring:
  elasticsearch:
    rest:
      # es server的地址
      uris: 192.168.0.102:9200
      # 连接超时时间
      connection-timeout: 6s
      # 访问超时时间
      read-timeout: 10s
```

### 定义实体类映射ES文档

类比于MyBatis-Plus可以定义实体类去映射数据库中的表中的数据，使用Spring Data Elasticsearch时，我们也可以通过定义一个实体类映射ES索引中的文档。

**注意@Id注解**  **这里的类型是后面继承ElasticsearchRepository接口时需要的第二个泛型的类型；也就是这个Long类型**

```java
@Data
@Document(indexName = "goods" , shards = 1,replicas = 0)  //一个分片，0个副本
public class Goods {
    // 商品Id skuId _id
    @Id
    private Long id;

    @Field(type = FieldType.Keyword, index = false)
    private String defaultImg;

    //  es 中能分词的字段，这个字段数据类型必须是 text！keyword 不分词！
    @Field(type = FieldType.Text, analyzer = "ik_max_word")
    private String title;
    
         .
         .
         .
             

    @Field(type = FieldType.Keyword)
    private String thirdLevelCategoryName;

    //  商品的热度！ 我们将商品被用户点查看的次数越多，则说明热度就越高！
    @Field(type = FieldType.Long)
    private Long hotScore = 0L;

    // 平台属性集合对象
    // Nested 支持嵌套查询
    @Field(type = FieldType.Nested)
    private List<SearchAttr> attrs;

}
```

```java
/*
   该类映射nested平台属性
*/
@Data
public class SearchAttr {
    // 平台属性Id
    @Field(type = FieldType.Long)
    private Long attrId;
    // 平台属性值名称
    @Field(type = FieldType.Keyword)
    private String attrValue;
    // 平台属性名
    @Field(type = FieldType.Keyword)
    private String attrName;
}
```

**注意：**在Goods类上，通过添加@Document注解，我们将Goods类映射的文档所属的索引：

- @Document注解的index属性，用来定义实体类所映射的文档所属的目标索引名称
- @Document的shards属性，表示目标索引的住分片数量
- @Document的replicas属性，表示每个主分片所拥有的副本分片的数量

**在Goods类的成员变量Id上通过添加@Id注解指定，Id成员变量映射到Goods索引中文档的id字段，同时也映射到文档的唯一表示_id字段。**

在Goods类的其他成员变量上，通过添加@Field注解，定义成员变量和文档字段的映射关系：

- 默认同名成员变量，映射到文档中的同名字段(也可以由@Field注解的name属性显示指定)
- 通过@Field注解的type属性指定文档中同名字段的数据类型
- 通过@Field注解的analyzer属性，指定成员变量所映射的文档字段所使用的的分词器



### xxxRepository继承ElasticsearchRepository

**定义一个接口去继承ElasticsearchRepository（其他的不用写），注入其对象后就可以快速进行crud（有内置的方法更简便）、分页、排序等简单操作 （上架、下架、更新热点）**

类比于Mybatis-Plus中定义BaseMaper子接口即可对单表做增删改查的操作，Spring Data Elastisearch中我们可以通过定义ElasticsearchRepository子接口，迅速实现对索引中的文档数据的增删改查，以及通过自定义方法，实现自定义查询。

```java
public interface GoodsRepository extends ElasticsearchRepository<Goods,Long> {

}
```

**ElasticsearchRepository**接口需要接收**两个泛型，第一个泛型即映射实体类，第二个泛型是在实体类中加了@Id注解的成员变量的数据类型，即映射到文档唯一标识_id字段的成员变量类型。**

一旦我们定义好了ElasticsearchRepository的子接口，马上就可以实现对goods索引中文档的增删改查功能

```java
    // 注入repository对象
    @Autowired
    private GoodsRepository goodsRepository;

    
    // 保存单个文档对象
    Goods good = ....
    goodsRepository.save(good);



    // 批量保存多个文档对象
    List<Goods> goods = ...
    goodsRepository.save(goods);

   // 根据id查询
   goodsRepository.findById(id);

   // 根据id删除
   goodsRepository.deleteById(id);
    
```

同时，我们还需要注意一点，一旦我们定义好了ElasticsearchRepository接口，而且被SpringBoot启动类扫描到，那么在应用**启动**的时候，如果ElasticsearchRepository子接口所访问的索引在ES中不存在，Spring Data Elasticsearch会在ES中**自动创建索引，并根据映射实体类定义索引的映射**。

Repository好用，但是具有一定的局限性，如果面对比较复杂的查询，此时就只能使用Spring Data Elasticsearch提供的另外一个工具ElasticsearchRestTemplate了。



### 引入ElasticsearchRestTemplate

为了构造更复杂的查询，引入ElasticsearchRestTemplate。 **直接@Autowired注入即可使用**。然后构造自定义分页，高亮，nested以及聚合查询，并发起请求

```java

    @Autowired
    ElasticsearchRestTemplate restTemplate;

    @Test
    public void testRestTemplate() {
        // 该Builder包含所有搜索请求的参数
        NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder();


        // 获取bool查询Builder
        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();


        // 构造bool查询中match查询
        MatchQueryBuilder matchQuery = QueryBuilders.matchQuery("title", "小米手机");
        // 将该查询加入bool查询must中
        boolQueryBuilder.must(matchQuery);

        TermQueryBuilder subQueryForAttrNested = QueryBuilders.termQuery("attrs.attrValue", "8G");
        // 构造nested查询
        NestedQueryBuilder attrsNestedQuery = QueryBuilders.nestedQuery("attrs", subQueryForAttrNested, ScoreMode.None);
        // 将nested查询作为一个过滤条件
        boolQueryBuilder.filter(attrsNestedQuery);


        // 将整个bool查询添加到NativeSearchQueryBuilder
        queryBuilder.withQuery(boolQueryBuilder);

        // 构造分页参数
        //PageRequest price = PageRequest.of(0, 10, Sort.by(Sort.Order.desc("price")));
        PageRequest price = PageRequest.of(0, 10);
        // 向NativeSearchQueryBuilder添加分页参数
        queryBuilder.withPageable(price);

        // 按照指定字段值排序
        FieldSortBuilder priceSortBuilder = SortBuilders.fieldSort("price").order(SortOrder.ASC);
        queryBuilder.withSort(priceSortBuilder);

        // 构造高亮参数
        HighlightBuilder highlightBuilder = new HighlightBuilder();
        highlightBuilder.field("title").preTags("<font color='red'>").postTags("</font>");
        // 向NativeSearchQueryBuilder添加高亮参数
        queryBuilder.withHighlightBuilder(highlightBuilder);

        // 设置品牌聚合(平台属性等的聚合也是相同的方式)
        TermsAggregationBuilder termsAggregationBuilder = AggregationBuilders.terms("tmIdAgg").field("tmId")
                .subAggregation(AggregationBuilders.terms("tmNameAgg").field("tmName"))
                .subAggregation(AggregationBuilders.terms("tmLogoUrlAgg").field("tmLogUrl"));


        // 向NativeSearchQueryBuilder添加聚合参数
        queryBuilder.addAggregation(termsAggregationBuilder);

        // 结果集过滤，只包含原始文档的id，defaultImg，title，price
        queryBuilder.withFields("id", "defaultImg", "title", "price");

        // 使用ElasticsearchRestTemplate发起搜索请求
        NativeSearchQuery build = queryBuilder.build();
        SearchHits<Goods> search = restTemplate.search(build, Goods.class);

        //封装所有的查询数据
        SearchResponseDTO searchResponseDTO = new SearchResponseDTO();

        // 获取满足条件的总文档数量
        long totalHits = search.getTotalHits();
        // 设置查询到的总文档条数
        searchResponseDTO.setTotal(totalHits);


        // 获取包含所有命中文档的SearchHit对象
        List<SearchHit<Goods>> searchHits = search.getSearchHits();


        // 处理搜索到的结果集即SearchHit<Goods>集合, 并使用高亮字符串替换
        List<GoodsDTO> goodsList = searchHits.stream().map(hit -> {
            // 获取命中的文档
            Goods content = hit.getContent();

            //获取高亮字段
            List<String> title = hit.getHighlightField("title");

            // 用高亮字段替换
            content.setTitle(title.get(0));
            // 将Goods对象转化为GoodsDTO对象
            GoodsDTO goodsDTO = goodsConverter.goodsPO2DTO(content);
            return goodsDTO;
        }).collect(Collectors.toList());

        // 设置查询到的结果列表
        searchResponseDTO.setGoodsList(goodsList);

        // 从品牌聚合中获取品牌集合

        // 根据id获取品牌id terms聚合结果
       Terms terms = search.getAggregations().get("tmIdAgg");
        List<SearchResponseTmDTO> trademarkList = terms.getBuckets().stream().map(tmIdBucket -> {
            // 封装品牌数据
            SearchResponseTmDTO searchResponseTmDTO = new SearchResponseTmDTO();

            String tmIdStr = tmIdBucket.getKeyAsString();
            // 获取品牌id
            Long tmId = Long.parseLong(tmIdStr);
            // 设置品牌id
            searchResponseTmDTO.setTmId(tmId);

            // 获取品牌名称聚合(子聚合)
            Terms tmNameAgg = tmIdBucket.getAggregations().get("tmNameAgg");
            // 通过聚合桶的名称获取品牌名称
            String tmName = tmNameAgg.getBuckets().get(0).getKeyAsString();
            // 设置品牌名称
            searchResponseTmDTO.setTmName(tmName);


            // 获取品牌logo聚合(子聚合)
            Terms tmLogoUrlAgg = tmIdBucket.getAggregations().get("tmLogoUrlAgg");
            // 通过聚合桶的名称获取品牌名称
            String tmLogoUrl = tmLogoUrlAgg.getBuckets().get(0).getKeyAsString();
            // 设置品牌名称
            searchResponseTmDTO.setTmLogoUrl(tmLogoUrl);

            return searchResponseTmDTO;
        }).collect(Collectors.toList());

        // 设置聚合品牌数据
        searchResponseDTO.setTrademarkList(trademarkList);
        
        // .....

    }
```

### ElasticsearchRestTemplate 的search()方法

`ElasticsearchRestTemplate` 是 Spring Data Elasticsearch 提供的一个类，用于与 Elasticsearch 进行交互。

**`search()` 方法是 `ElasticsearchRestTemplate` 类中的一个方法，用于执行 Elasticsearch 的搜索操作。它接受一个 `SearchQuery` 对象作为参数，并返回一个 `SearchHits` 对象，该对象包含了搜索结果中的命中（hits）信息。**

**`SearchQuery` 是一个接口，用于构建 Elasticsearch 的查询条件。你可以使用 `SearchQuery` 的实现类，如 `NativeSearchQuery` 或 `CriteriaQuery`，来构建不同类型的查询。这些查询可以包含各种条件、过滤器、排序等。**

**当调用 `search()` 方法时，`ElasticsearchRestTemplate` 会将 `SearchQuery` 对象转换为 Elasticsearch 的查询语句，并发送给 Elasticsearch 服务器执行搜索操作。然后，它会解析 Elasticsearch 返回的结果，并将结果封装在 `SearchHits` 对象中返回。**

你可以使用 `SearchHits` 对象获取搜索结果的详细信息，如总命中数、每个命中文档的得分、文档的字段值等。

需要注意的是，`ElasticsearchRestTemplate` 是基于 Elasticsearch 的 REST API 进行操作的，因此在使用之前，你需要配置好 Elasticsearch 的连接信息，包括主机、端口等。

这些是关于 `ElasticsearchRestTemplate` 的 `search()` 方法的基本说明，具体的用法和更多细节可以参考 Spring Data Elasticsearch 的文档或者相关的 API 文档。



## 热度排名

逻辑是这样的：**1.** 当你搜索商品，点进商品详情页之后，**商品服务**就会远程**调用 搜索服务**的增加热度方法；**2.** 搜索服务就会先将热度数据保存到Redis中（因为点击量大，减小压力）; **3.** 等到比如有10次了，然后再写入到ES中。**4.** ES在自定义的搜索条件中， 会有根据热度降序排序，这样就完成了热度排名。



## 共享会话解决单点登录的思路    [单点登录](D:\Java\java50th\java50-course-materials\04-微服务\01-课件\17_单点登录\单点登录.md)

### 解决思路

这个思路的核心要点，当用户登录成功之后，将用户的会话信息，存储在一个共享的地方(比如Redis)，而不是某个服务的内存中，这样一来就可以通过访问共享会话来判断用户的登录状态。



**对于用户服务而言:**

- 一旦判断用户登录成功，就将用户信息存储到Redis，这就相当于将用户登录之后的会话，存储到Redis中。
- 同时，随机生成Token，作为Redis中，会话信息的key，通过该Token获取Redis中对应的共享会话信息
- 该Token还需要返回给前端。

**对于前端而言**，只要用户登陆成功，就会接收到Redis中，登录会话对应的Token，前端就会在请求Cookie中携带该Token，发起请求的时候，会在请求中携带该Cookie。

**对于服务而言：**

- 因为所有的请求都是经过网关的，所以登录身份认证的工作，没有必要在每个服务中完成，而是统一在网关中完成即可
- 所以网关会拦截那些需要做登录身份认证的请求，并根据请求中携带是否携带Cookie，以及通过Cookie中的Token是否能在Redis中获取登录会话，来判断用户是否登录过。

### 验证逻辑

```java
// 全局过滤器，对请求进行拦截校验，逻辑是什么呢？
    // 1. 获取当前访问的Path
    // 2. 判断当前这个path是否需要登录
    // 3. 如果需要登录，那么判断用户是否已经登录
    // 4. 如果 当前path需要登录且 未登录，拦截
    // 5. 其他的情况，放行    
          // a) 当前访问的path 不需要登录
          // b) 需要登录且用户已经登录
```



**补充：**Nginx 的 IP Hash 策略可以在一定程度上解决单点登录问题。通过使用 IP Hash 策略，当用户进行认证并获得一个会话后，该会话将被绑定到一个特定的后端服务器，即使用户的后续请求经过负载均衡也会被路由到同一台服务器上。这样可以确保用户的会话状态在整个系统中保持一致，从而实现单点登录的效果。需要注意的是，Nginx 的 IP Hash 策略并不能解决所有与单点登录相关的问题，特别是在分布式系统中，还需要考虑会话的同步、跨域访问等其他方面的问题。因此，在实际应用中，可能需要结合其他技术和策略来完善单点登录的解决方案。



## RocketMQ 

RocketMQ 是一个开源的分布式消息中间件系统，由阿里巴巴集团开发和维护。它具有高吞吐量、低延迟、高可靠性和强大的扩展性，广泛应用于大规模分布式系统中的消息通信场景。

RocketMQ 的核心概念包括：

1. Producer（生产者）：负责发送消息到 RocketMQ 的消息队列中。

2. Consumer（消费者）：从 RocketMQ 的消息队列中接收和处理消息。

3. Topic（主题）：消息的逻辑分类，类似于消息的主题或者标签。

4. Message Queue（消息队列）：每个主题可以被划分为多个消息队列，用于存储消息。

5. Broker（消息代理）：负责存储消息，接收和传递消息的中间件组件。

RocketMQ 提供了丰富的特性，包括：

- 可靠的消息传递：RocketMQ 提供了多种消息传递模式，包括同步、异步和单向传递，可以根据需求选择不同的传递方式。

- 高吞吐量和低延迟：RocketMQ 的设计目标之一是实现高吞吐量和低延迟的消息传递，适用于高并发场景。

- 消息顺序性：RocketMQ 支持按照消息的顺序进行传递和处理，保证消息的有序性。

- 消息事务：RocketMQ 支持消息事务，允许在消息发送和消息确认之间执行本地事务。

- 消息过滤：RocketMQ 提供了灵活的消息过滤机制，可以根据消息的属性进行过滤，只选择感兴趣的消息进行处理。

RocketMQ 可以用于构建各种分布式系统中的消息通信模块，例如订单系统、支付系统、日志系统等。它有着广泛的社区支持和活跃的开发维护，可以满足大规模分布式系统中的消息传递需求。

## RocketMQ的代码中的实际用法



1. 在**common服务**里有个一个BaseProducer里面有init（）方法，sendMessage（）方法，sendDelayMessage（）方法。

2. 在**商品服务**的SkuServiceImpl中**onSale（）**方法里，会调用一个common服务的BaseProducer来生产一个**上架消息**(注入进来，调用相应的方法即可)，**offSale（）**方法会发送一个**下架消息**。

3. 在**搜索服务**里会单独写两个消费者类，创建消息消费者，然后设置相同的注册中心，订阅相同的主题，然后再设置好消息监听器，监听到上架的消息后，便会调用searchService中的upperGoods（）方法，用消息中的skuId，将相应的商品储存es中。

4. 延迟消息是用在订单服务里， 超时30分钟自动取消订单



## yaml配置文件的解读示例

```yaml
spring:
  application:
    name: server-gateway
  profiles:
    active: dev
  cloud:
    nacos:
      discovery:
        server-addr: 120.46.174.115:8848
      config:
        server-addr: 120.46.174.115:8848
        file-extension: yaml
        shared-configs:
          - data-id: common.yaml
        timeout: 10000
# 网关会使用 配置中心里面哪些配置文件？
# common.yaml
# server-gateway-dev.yaml
```

**解读：**

这段 YAML 配置文件用于配置 Spring Boot 应用程序的属性和 Nacos 服务注册与配置中心。

```yaml
spring:
  application:
    name: service-product
```

- `spring.application.name`：指定 Spring Boot 应用程序的名称为 "service-product"。这个名称将用于服务注册和其他相关配置。

```yaml
profiles:
  active: dev
```

- `profiles.active`：指定当前活动的配置文件为 "dev"。这表示应用程序将使用 "dev" 配置文件中的属性和配置。
- **dev**表示开发环境，**test**表示测试环境，**prod**表示生产环境

```yaml
cloud:
  nacos:
    discovery:
      server-addr: 120.46.174.115:8848
    config:
      server-addr: 120.46.174.115:8848
      file-extension: yaml
      shared-configs:
        - data-id: common.yaml
```

- `cloud.nacos.discovery.server-addr`：指定 Nacos 服务注册中心的地址为 "120.46.174.115:8848"。这将告诉应用程序在此地址上注册和发现服务。
- `cloud.nacos.config.server-addr`：指定 Nacos 配置中心的地址为 "120.46.174.115:8848"。这将告诉应用程序从此地址获取配置属性。
- `cloud.nacos.config.file-extension`：指定配置文件的扩展名为 "yaml"。这表示应用程序将从 Nacos 配置中心读取以 YAML 格式存储的配置文件。
- `cloud.nacos.config.shared-configs`：指定共享的配置文件列表。在这个例子中，只有一个共享配置文件 "common.yaml"。

总结起来，这个配置文件指定了应用程序的名称为 "service-product"，活动配置文件为 "dev"，并设置了 Nacos 的服务注册和配置中心地址。它还指定了应用程序将从 Nacos 配置中心读取以 YAML 格式存储的共享配置文件 "common.yaml"。



## 商品服务将公共服务当作依赖进入了进来，那么公共服务里面的@value注解，就会从商品服务的配置文件中读取想要的值对吗？公共服务里没有配置文件

是的，如果商品服务将公共服务作为依赖注入，并且公共服务中使用了 `@Value` 注解来注入配置文件中的值，那么公共服务将会从商品服务的配置文件中读取相应的属性值。

在 Spring 中，属性值的注入是基于容器的，而不是基于类。当商品服务将公共服务作为依赖注入时，公共服务也会成为 Spring 容器中的一个组件。因此，当公共服务使用 `@Value` 注解时，它将查找 Spring 容器中的配置文件，并读取相应的属性值。

这意味着，公共服务可以通过 `@Value` 注解来访问商品服务的配置文件中的属性值，而不需要自己拥有独立的配置文件。这种方式可以方便地将公共配置与特定服务的配置分离，使得公共服务更加灵活和可复用。

需要确保商品服务的配置文件中包含了公共服务所需的属性值。如果商品服务的配置文件中没有相关的属性定义，那么公共服务将无法正确注入属性值。



## redis相关问题

项目中redis是在common服务里，进行了基本配置，构造了RedissonClient对象，并且进行了@bean的注入

### redis使用场景

- 缓存：穿透、击穿、雪崩 ； 双写一致性、持久化 ； 数据过期、淘汰策略

- 分布式锁： setnx、redisson

- 保存token：string类型

- 消息队列：List类型

- 延迟类型：zset类型

- 计数器



### （购物车问题）如果我当前的数据触发了redis的持久化策略写入到了磁盘上；然后我内存里的该数据被删除了，等到下次我再查询该数据的时候redis会怎么操作

如果你在 Redis 中设置了持久化策略，并且数据已经被成功写入到磁盘上，当你在内存中删除了该数据后，当你再次查询该数据时，Redis 会按照以下步骤进行操作：

1. 当你查询已被删除的数据时，Redis 会发现该键在内存中不存在。

2. Redis 会检查持久化文件（RDB 文件）中是否存在该键的数据。由于你之前已经触发了持久化策略并将数据写入磁盘，所以 Redis 可能会在持久化文件中找到该键的数据。

3. 如果 Redis 在持久化文件中找到了该键的数据，它会将数据从磁盘加载到内存中，并返回给你查询结果。

需要注意的是，如果你在查询已被删除的数据之前进行了 Redis 服务器重启，Redis 会重新加载持久化文件并将数据恢复到内存中。在这种情况下，即使你已经在内存中删除了该数据，Redis 仍然可以从持久化文件中恢复数据并返回给你查询结果。

因此，Redis 的持久化机制可以确保即使在数据被删除后，仍然可以从磁盘中恢复数据，并且保证数据的持久性和可靠性。

### redis队列（用List数据结构），解决商品超卖问题

****

1. 将元素添加到队列中，可以使用 Redis 的 **LPUSH** 或 **RPUSH** 命令。LPUSH 将元素添加到队列的头部，RPUSH 将元素添加到队列的尾部。
2. 从队列中获取元素，可以使用 Redis 的 **LPOP** 或 RPOP 命令。LPOP 从队列的头部移除并返回元素，RPOP 从队列的尾部移除并返回元素。



使用 Redis 实现队列时，需要**注意**以下几点：

- 队列的元素可以是任意类型的数据，如字符串、数字、对象等。
- 队列是持久化的，即使 Redis 重启，队列中的数据也不会丢失。
- Redis 的 List 数据结构是线程安全的（**原子性**），多个客户端可以同时对队列进行操作。





## 购物车流程



使用redis的**hash数据结构**（hset key field value）来存储用户的购物车,  **key存用户的id； field存商品id； value存CartInfoDTO**



**1. 添加购物车**操作，直接用redissonClient.getMap()方法，就可以得到这个用户的整个购物车。判断是否已经有传入的商品id了， 如果有就更新数量，没有的话就添加商品到购物车。

**2. 查询购物车**的话有两点需要注意：

- 用户先用临时id添加了购物车，然后登陆正式id，那么需要将临时的购物车和正式的购物车合并起来，然后删掉临时的购物车

- 购物车列表排序要按照**添加时间**进行**排序**





### 购物车扣库存问题

加入购物车不扣，生成订单时也不扣，完成付款时才扣库存。





## nacos

1. 项目中只需要引入依赖
2. 在每个服务的yaml文件中配置nacos的地址（包括注册中心和配置中心）
3. 然后在spring boot应用的启动类上添加`@EnableDiscoveryClient`注解即可





## Ribbon

nacos 已经整合了ribbon，配置文件中设置相关策略



关于负载均衡策略，如果你没有显式地配置 `NFLoadBalancerRuleClassName`，那么 Ribbon 将会使用默认的负载均衡规则。默认规则是 `ZoneAvoidanceRule`，它会根据服务实例所在的可用区域进行负载均衡。



### Ribbon负载均衡

Ribbon是一个客户端负载均衡器，能够给HTTP客户端带来灵活的控制。**其实现的核心功能，就是一组选择策略，帮助我们在一个服务集群中，选择一个服务实例，并向该实例发起调用请求。**它所支持的负载均衡策略如下:

| 策略               | 实现类                        | 描述                                                         |
| ------------------ | ----------------------------- | ------------------------------------------------------------ |
| **随机策略**       | RandomRule                    | 随机选择server                                               |
| 轮训策略           | RoundRobinRule                | 轮询选择                                                     |
| 重试策略           | RetryRule                     | 对选定的负载均衡策略(轮训)之上重试机制，在一个配置时间段内当选择服务不成功，则一直尝试使用该策略选择一个可用的服务； |
| 最低并发策略       | BestAvailableRule             | 逐个考察服务，如果服务断路器打开，则忽略，再选择其中并发连接最低的服务 |
| 可用过滤策略       | AvailabilityFilteringRule     | 过滤掉因一直失败并被标记为circuit tripped的服务，过滤掉那些高并发链接的服务（active connections超过配置的阈值) |
| 响应时间加权重策略 | WeightedResponseTimeRule      | 根据server的响应时间分配权重，响应时间越长，权重越低，被选择到的概率也就越低。响应时间越短，权重越高，被选中的概率越高，这个策略很贴切，综合了各种因素，比如：网络，磁盘，io等，都直接影响响应时间 |
| **区域权重策略**   | ZoneAvoidanceRule（**默认**） | 综合判断服务所在区域的性能，和服务的压力，轮询选择server并且判断一个AWS Zone的运行性能是否可用，剔除不可用的Zone中的所有server |

### 自定义负载均衡策略

两种方式：

1. 创建类实现IRule接口，可以指定负载均衡策略（全局）
2. 在客户端的配置文件中，可以配置**某一个服务**调用的负载均衡策略（局部）

## nginx  （反向代理、负载均衡、动静分离）

**补充**：配置文件目录

Nginx的配置文件通常位于`/etc/nginx`目录下。具体的配置文件路径可能有所不同，取决于您的操作系统和Nginx的安装方式。

以下是一些常见的Nginx配置文件路径：

- 主配置文件：`/etc/nginx/nginx.conf`
- 站点配置文件目录：`/etc/nginx/conf.d/` 或 `/etc/nginx/sites-available/`
- 默认站点配置文件：`/etc/nginx/conf.d/default.conf` 或 `/etc/nginx/sites-available/default`

您可以使用命令行浏览文件系统以找到Nginx的配置文件。运行以下命令可以列出`/etc/nginx`目录下的文件和文件夹：
```
ls /etc/nginx
```

请注意，您可能需要以超级用户（如root）身份运行此命令，或者使用`sudo`命令。

一旦找到了Nginx的配置文件，您可以使用文本编辑器（如`vi`或`nano`）打开它以进行修改。请确保在修改配置文件之前备份原始文件，以防止意外的更改导致问题。

如果您使用的是自定义的Nginx安装路径，则配置文件可能位于不同的位置。在这种情况下，您可以尝试运行`nginx -t`命令来检查Nginx配置文件的位置和语法错误。

-----------------------------------------------



正向代理代理客户端 ； **反向代理代理代理服务端**   

并发性好 ，官方说可以有50000个请求， tomcat 也就4、500个请求。



**集群及负载均衡配置**    nginx负载均衡

```
#负载均衡策略 
# 1 轮询（默认）
# 2 weight
# 3 ip_hash
# 4 least_conn 最少连接方式
# 5 fair(第三方) 响应时间
# 6 url_hash (第三方) 
```

如：最少连接

```ini
#最少连接
upstream backend {
    least_conn;
    server backend-server1;
    server backend-server2;
    # 添加更多后端服务器...
}
```

权重

```ini
#权重
upstream backend {
    server backend-server1 weight=3;
    server backend-server2 weight=2;
    # 添加更多后端服务器...
}
```



--------------------------------------

**反向代理的配置**    **(重要)**

```bash
http{
	...
	...
	#负载均衡策略  mysvr在这里是一个自定义的名字，你可以随意命名。它代表了一个后端服务器组，用于负载均衡。 此处是权重  
	upstream mysvr{
		#服务器资源
		server 192.168.45.151:8080 weight=2;
		server 192.168.45.151:8081 weight=1;
	}
	
  #这部分是nginx作为反向代理服务器的配置
  server{
  	  #nginx监听的端口
      listen  80;
      #虚拟服务器的识别标记，一般配置为本机ip
      server_name 192.168.45.151;
	  #代理设置地址
      location / {
          proxy_pass http://mysvr; 
      }
      
       # 定义静态资源的访问路径和目录  /path/to/static/files 是静态资源文件的存储路径，需要将其替换为实际的路径。
       location /static {
          alias /path/to/static/files;
      }
    
    
  }
  
  #可以配置多个server 比如再配置一个监听443端口
  server{
   ...
  }
  
}
```



## 高并发的一些解决方案以及秒杀相关

### redis缓存

**缓存预热**，商品秒杀时，提前将相关的数据放入缓存中。

缓存预热方案落地：

- 什么时候存入Redis？凌晨放入当天所有的秒杀商品数据
- 如何实现？**Spring Scheduling 定时任务**

### 消息队列，限流削峰

在高并发场景下把请求存入消息队列，利用排队思想降低系统瞬间峰值

具体场景：购物网站开展秒杀活动，一般由于瞬时访问量过大，服务器接收过大，会导致流量暴增，相关系统无法处理请求甚至崩溃。而加入消息队列后，系统可以从消息队列中取数据，相当于消息队列做了一次缓冲。

![img](D:\Java\java50th\java50-course-materials\04-微服务\01-课件\19. RocketMQ\RocketMQ课件.assets\1506330074201_6290_1506330076616.png)

**优点**： 

1. **请求先入消息队列**，而不是由业务处理系统直接处理，做了一次缓冲,极大地减少了业务处理系统的压力；
2. 事实上，秒杀时，后入队列的用户无法秒杀到商品，这些**请求可以直接被抛弃**，返回活动已结束或商品已售完信息；

## MybatisPlus    [mybatisplus](D:\Java\java50th\java50-course-materials\04-微服务\01-课件\07_基础技术\基础技术.md)

### 先引入依赖，再配置文件中添加配置

创建一个SpringBoot工程(可以使用Spring Initializer)，引入如下依赖

```maven
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
        <scope>test</scope>
    </dependency>
    <dependency>
        <groupId>com.baomidou</groupId>
        <artifactId>mybatis-plus-boot-starter</artifactId>
        <version>3.5.3.1</version>
    </dependency>
     <dependency>
         <groupId>mysql</groupId>
         <artifactId>mysql-connector-java</artifactId>
         <scope>runtime</scope>
     </dependency>
      <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
</dependencies>
```

在application.yml中添加如下配置

```yaml
spring:
  datasource:
    url: jdbc:mysql://127.0.0.1:3306/test?characterEncoding=utf8&serverTimezone=GMT%2B8&useSSL=false
    driver-class-name: com.mysql.cj.jdbc.Driver
    username: root
    password: 1234
# 在日志中显示实际执行的sql语句
logging:
  level:
    com.cskaoyan.mybatis.plus.mapper: debug
```

### 在启动类上加MapperScan注解

```java
@SpringBootApplication
@MapperScan("com.cskaoyan.mybatisplus.mapper")
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
```

### 定义user表对应的映射实体类User

```java
@Data //lombok注解
public class User {
    private Long id;
    private String name;
    private Integer age;
    private String email;
}
```

### 定义Mapper

**继承一个BaseMapper，泛型里面为要操作的实体类型**

BaseMapper是MyBatis-Plus提供的模板mapper，其中包含了基本的CRUD方法，泛型为操作的实体类型

```java
public interface UserMapper extends BaseMapper<User> {
}
```

### 分页操作

MyBatis Plus自带分页插件，只要简单的配置即可实现分页功能

添加配置类

```java
@Configuration
//@MapperScan("com.cskaoyan.mybatisplus.mapper") //可以将主类中的注解移到此处
public class MybatisPlusConfig {
    @Bean
    public MybatisPlusInterceptor mybatisPlusInterceptor() {
        MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor();
        interceptor.addInnerInterceptor(new PaginationInnerInterceptor(DbType.MYSQL));
        return interceptor;
	}
}
```

```java
@Test
public void testPage(){
    //设置分页参数
    // 注意页数从1开始
    Page<User> page = new Page<>(1, 2);   //1表示要查询的页码，2表示每页显示的记录数
    userMapper.selectPage(page, null);
    //获取分页中的每一条记录
    List<User> list = page.getRecords();
    list.forEach(System.out::println);
    System.out.println("当前页：" + page.getCurrent());
    System.out.println("每页显示的条数：" + page.getSize());
    System.out.println("总记录数：" + page.getTotal());
    System.out.println("总页数：" + page.getPages());
    System.out.println("是否有上一页：" + page.hasPrevious());
    System.out.println("是否有下一页：" + page.hasNext());
}
```

## （待补充）io流





## 多线程

### 实现方式一： 继承Thread类 重写run（）方法



### 实现方式二： 实现Runnable接口



### 方式比较

在Java中，实现多线程有两种常见的方式：继承Thread类和实现Runnable接口。这两种方式有以下区别：

1. 继承Thread类：
   - 优点：继承Thread类可以直接重写Thread类的run()方法，使得代码结构相对简单，易于理解和维护。
   - 缺点：由于Java不支持多继承，因此继承Thread类后，该类无法再继承其他类，限制了类的扩展性。此外，如果想要实现多线程，必须创建Thread类的实例，可能导致资源的浪费。

2. 实现Runnable接口：
   - 优点：实现Runnable接口可以避免继承的限制，因为Java支持实现多个接口。此外，实现Runnable接口还可以使得代码具有更好的可重用性，因为多个线程可以共享同一个Runnable实例。
   - 缺点：相对于继承Thread类，实现Runnable接口需要单独创建一个Thread实例，并将Runnable实例作为参数传递给Thread构造函数。这增加了一些额外的代码和复杂性，使得代码稍微复杂一些。

总体而言，选择继承Thread类还是实现Runnable接口取决于具体的需求和设计考虑。如果只是简单地创建一个线程并执行某些任务，继承Thread类是比较方便的。但如果需要更好的代码扩展性和可重用性，以及避免单继承的限制，实现Runnable接口是更好的选择。此外，还可以使用Java 8引入的函数式接口`java.util.concurrent.Callable`和`java.util.concurrent.Future`来实现多线程，这种方式更加灵活和强大。在Java中，实现多线程有两种常见的方式：继承Thread类和实现Runnable接口。这两种方式有以下区别：

1. 继承Thread类：
   - 优点：继承Thread类可以直接重写Thread类的run()方法，使得代码结构相对简单，易于理解和维护。
   - 缺点：由于Java不支持多继承，因此继承Thread类后，该类无法再继承其他类，限制了类的扩展性。此外，如果想要实现多线程，必须创建Thread类的实例，可能导致资源的浪费。

2. 实现Runnable接口：
   - 优点：实现Runnable接口可以避免继承的限制，因为Java支持实现多个接口。此外，实现Runnable接口还可以使得代码具有更好的可重用性，因为多个线程可以共享同一个Runnable实例。
   - 缺点：相对于继承Thread类，实现Runnable接口需要单独创建一个Thread实例，并将Runnable实例作为参数传递给Thread构造函数。这增加了一些额外的代码和复杂性，使得代码稍微复杂一些。

总体而言，选择继承Thread类还是实现Runnable接口取决于具体的需求和设计考虑。如果只是简单地创建一个线程并执行某些任务，继承Thread类是比较方便的。但如果需要更好的代码扩展性和可重用性，以及避免单继承的限制，实现Runnable接口是更好的选择。此外，还可以使用Java 8引入的函数式接口`java.util.concurrent.Callable`和`java.util.concurrent.Future`来实现多线程，这种方式更加灵活和强大。



## 反射

### 什么是反射

获取运行时类信息的一种手段

反射的起点是字节码文件对象

### 获取字节码文件对象的几种方式

- 对象.getClass()
- 类名.class
- Class.forName(String className)  全限定名
- ClassLoader里的loadClass(String className)

**注意:**

无论通过什么方式获取的字节码文件对象 都是同一个

###  项目中那里用到了反射

1.springMVC的实现就用到了反射

**2.在使用aop时：**

是的，methodSignature.getMethod().getAnnotation(RedisCache.class) 这一句使用了反射机制。

首先，methodSignature.getMethod() 返回了当前方法的 Method 对象。然后，通过 Method 对象的 getAnnotation() 方法，可以获取到方法上标注的注解对象。

在这个例子中，假设 RedisCache 是一个自定义的注解，**通过 getAnnotation(RedisCache.class) 可以获取到当前方法上标注的 RedisCache 注解对象。（通过获取这个注解对象的前缀，最终可以去访问redis中的数据，如果有就返回，没有就加锁访问数据库）**

通过反射，我们可以在运行时获取到方法的相关信息，包括注解信息。这样可以方便地根据注解来做一些特定的处理或逻辑判断。

```java
RedisCache redisCache = methodSignature.getMethod().getAnnotation(RedisCache.class);
```

## 注解

**项目中redis的自定义注解，见上文**

主要是两个

@Target和 @Retention

```java
@Target({ElementType.METHOD})         // 注解可以作用的目标
@Retention(RetentionPolicy.RUNTIME)   // 注解的保留级别
public @interface RedisCache {

    // 给缓存数据增加前缀，以区分不同的缓存数据
    String prefix() default "cache:";

}
```

## （待补充）关键字查询功能











## shiro安全框架

### 总结

还有需要注意的是对Shiro框架的使用 ,能够实现权限的管理,  对于还没有登陆的用户, 就不能使用购物车和收藏,下单等业务, 但是他要能够,进行商品首页以及商品详情的查看, 这是之前项目不曾涉及到的新的且很重要的知识点, 写相应的业务代码时, 要非常清楚知道用户, 角色, 权限之间的关系, 以及相关业务涉及到那些权限 ; 然后在一些接口中, 没有传入具体的数据, 但是, 这个业务明显是和具体的用户有关的, 这时候我们需要通过他的认证信息来获得自己想要的参数, 要注意Subject的重要性, 使用subject.getPrincipals() , 来获取认证后的用户信息; Realms也是很重要的, 能够获得对应的认证信息和授权信息

### 核心术语

<span style='color:yellow;background:red;font-size:文字大小;font-family:字体;'>**Subject**</span>，主体，在Shiro中所做的几乎所有操作都基于当前正在执行的用户<span style='color:red;background:white;font-size:13px;font-family:楷体;'>**也就是基本上Shiro的操作都是使用Subject操作的，Subject指的就是当前操作的用户**</span>。在代码中的任何位置都可以轻松获得Subject，通过Subject可以方便的操作Shiro。比如我们可以使用Subject提供的方法来执行认证、判断是否认证等操作

![image-20230421104748250](D:\Java\java50th\java50-course-materials\03-JavaEE&Spring框架\02-笔记\image\Day26-shiro课程\image-20230421104748250.png)

<font color='red'>**Principals**</font>，主体鉴定后的参数<span style='color:red;background:white;font-size:13px;font-family:楷体;'>**也就是认证后的用户信息**</span>，可以是姓名、用户id、用户对象等形式

- 它是可以通过Subject来获得→ subject.getPrincipals();

<font color='red'>**Credentials**</font>，用来验证身份的秘密数据，通常指密码，生物数据<span style='color:red;background:white;font-size:13px;font-family:楷体;'>**比如指纹、面部、瞳孔等**</span>

<span style='color:yellow;background:red;font-size:文字大小;font-family:字体;'>**Realms**</span>，域或领域，安全的特殊数据存储对象（DAO），Shiro中的Realm主要是让你能够获得对应的认证信息和授权信息

Token，令牌，Shiro中的Token是作为登录操作的参数，subject.login(token)



## sms短信服务（阿里云）

**引入依赖，把一些值放入配置文件中。**



既然我们使用的是SpringBoot那么我们其实是可以使用配置文件来管理这些值的

比如我们创建一个`application-aliyun.yml`我们在其中提供对应的值，提供的值我们直接给容器中的组件使用，我们可以考虑使用Properties组件来接收这些值

```java
@Component
@ConfigurationProperties("wd.aliyun")
@Data
public class WdAliyunProperties {
  String accessKeyId;//wd.aliyun.access-key-id
  String accessKeySecret;//wd.aliyun.access-key-secret
  Oss oss;
  Sms sms;

  @Data
  public static class Oss {
    String bucket; //wd.aliyun.oss.bucket
    String endPoint;
    String urlPrefix;
  }

  @Data
  public static class Sms {
    String signName;
    String templateCode;
  }
}
```

我们就可以在配置文件中提供值

```yaml
# application-aliyun.yml
wd:
  aliyun:
    access-key-id: LTAI5t8gpxPTCR6W58RnZq4u
    access-key-secret: mVPbpbxia0JQotb7HyJAREV8QUuq8h
    oss:
      bucket: wdproject2
      end-point: oss-cn-beijing.aliyuncs.com
      url-prefix: https://${wd.aliyun.oss.bucket}.${wd.aliyun.oss.end-point}
    sms:
      sign-name: 王道训练营
      template-code: SMS_173765187
```

那么我们在容器中的其他组件中就可以引入对应的值

```java
@Service
public class OssServiceImpl implements OssService{
    @Autowired
    WdAliyunProperties aliyunProperties;
    @Override
    public OssPutResult save(MultipartFile file)  {
        String accessKeyId = aliyunProperties.getAccessKeyId();
        String accessKeySecret = aliyunProperties.getAccessKeySecret();
        String endpoint = aliyunProperties.getOss().getEndPoint();
        String bucketName = aliyunProperties.getOss().getBucket();
        String urlPrefix = aliyunProperties.getOss().getUrlPrefix();
    }
}
```



## git 常用命令

###  clone

```shell
 # 下载远程仓库的内容，并且在本地创建一个和远程仓库名同名的文件夹
git clone https://gitee.com/common-zhou/test_50th.git

# 下载到指定的文件夹中。文件夹需要是个空目录；或者这个文件夹不存在。都可以
git clone https://gitee.com/common-zhou/test_50th.git test_50th2

git clone https://gitee.com/ciggar/test-40th.git dirName
```

在git中管理文件的版本，需要使用文本文件。

.txt .md ;  不要使用docx pptx

### status

git status: 查看工作区和缓冲区的变化

### add

这个命令可以帮助我们把工作区中的变化提交到缓冲区。

```shell
# 有以下的三种提交方式

# 文件的名字
git add fileName

# 文件的类型，通配符添加
# 只添加 .java文件结尾的文件 。从工作空间提交到缓冲区
git add *.java

# 所有文件
git add .
```

### 分支操作

分支操作

```shell
# 查看所有分支
$ git branch

# 创建分支并切换 iss53
$ git checkout -b iss53

# 切换分支
$ git checkout iss53

# 合并分支
$ git merge iss53
```

<span style=color:red;background:yellow>**工作过程中的一般使用步骤**</span>

```SHELL
#  一般会有一个master 主分支
#  会有一个dev分支

# 1. 如果有需求，会从dev拉一个分支，比如 dev-feature1，所有的提交都提交在这个分支上
# 1.1 从dev拉取一个分支，并切换到这个分支
git checkout -b dev-feature1

# 2. 等到开发完成，会把这个分支合并到dev。 dev经过测试，会合并到master
# 2.1先切换到dev分支
git checkout dev 

# 2.2合并刚刚的分支
git merge dev-feature1
```

### idea上的git操作

<font color=red>**如果项目被git追踪了，则idea中会有对应的颜色提示。**</font>

红色代表是新增的文件

蓝色代表是文件有改动

绿色代表已经提交。其他的操作与git基本操作一致。



## （待补充）数据库优化

### 如何定位慢查询

#### 方案一：开源工具

- 调试工具：Arthas
- 运维工具：Prometheus、Skywalking

#### 方案二：Mysql自带慢日志

慢查询日志记录了所有执行时间超过指定参数（long_query_time, 单位：秒，默认10秒）的所有SQL语句的日志

如果要开慢查询日志，需要在Mysql的配置文件（/etc/my.cnf）中配置如下信息：

```
# 开启Mysql慢日志查询开关
slow_query_log=1
# 设置慢日志的时间为2秒，SQL语句执行时间超过2秒，就会视为慢查询，记录慢查询日志
long_query_time=2
```

配置完毕之后，通过以下指令重新启动Mysql服务器进行测试，查看慢日志文件中记录的信息

/var/lib/mysql/localhost-slow.log

**注意**：在mysql中开启了慢日志查询，我们设置的值是**2秒**，一旦sql执行超过了2秒就会记录到日志中（调试阶段）（生产环境不会开启，因为会损失性能）

---------------------------

**补充：**

是的，将`slow_query_log=1`添加到MySQL配置文件中会启用慢查询日志功能。当您将该选项设置为1时，MySQL会开始记录执行时间超过`long_query_time`设置的查询语句到慢查询日志中。

请注意，启用慢查询日志可能会对MySQL服务器的性能产生一定影响，因为它需要额外的系统资源来记录和处理查询。确保在生产环境中仅在必要时启用慢查询日志，并根据需要进行适当的调整和优化。

此外，除了设置`slow_query_log=1`，您还可以使用其他相关选项来定制慢查询日志的行为，例如：

- `slow_query_log_file`：指定慢查询日志文件的路径和名称。
- `long_query_time`：定义查询执行时间超过多少秒才被认为是慢查询。
- `log_queries_not_using_indexes`：记录未使用索引的查询语句。
- `log_slow_admin_statements`：记录管理员执行的慢查询语句。

您可以通过编辑MySQL配置文件并重新启动MySQL服务器来应用这些更改。确保在进行任何更改之前备份您的配置文件，以防止意外的问题发生。

----------------

### 找到了慢的sql，如何分析

可以采用**explain或者desc**命令获取mysql如何执行select语句的信息

如：**explain** select * from student where id = 1;

![image-20231225124934074](C:\Users\张仕谦\AppData\Roaming\Typora\typora-user-images\image-20231225124934074.png)

- possible_keys 当前sql可能会使用到的索引

- **key** 当前sql实际命中的索引   ：通过key和key_len这两个查看是否可能会命中索引

- **key_len** 索引占用的大小

- extra 额外的优化建议

  - Using where, Using Index 查找使用了索引，需要的数据在索引列中能找到，不需要回表查询数据
  - Using index condition 查找使用了索引，但是需要回表查询数据

- **type** 这条sql的连接的类型，性能由好到坏为NULL，system，const，eq_ref, ref, range, index, all

  - system : 查询系统中的表（平时比较少遇到）
  - const : 根据主键查询
  - eq_ref : 主键索引查询或唯一索引查询
  - ref : 索引查询
  - range ：范围查询
  - index ：索引树扫描       性能很差了
  - all ：全盘扫描

  ```ini
  q&a
  q: 这个sql语句执行很慢，如何分析呢？
  
  a：如果一条sql执行的很慢的话，我们通常会使用mysql自动的执行计划explain来查看这条sql的执行情况，比如在这里面可以通过key和key_len检查是否命中了索引，如果本身已经添加了索引， 也可以判断索引是否有失效的情况， 第二个，可以通过type字段查看sql是否有进一步优化的空间，是否存在全索引扫描或全盘扫描的，第三个可以通过extra建议来判断，是否出现了回表，如果出现了，可以尝试添加索引或修改返回字段来修复。
  ```




### 了解过索引吗（什么是索引）

**索引是数据库中一种用于提高检索数据速度的数据结构。**

- 索引是帮助mysql高效获取数据的数据结构（有序）
- 提高数据检索的效率，降低数据库的IO成本（不需要做全表扫描）
- 通过索引列对数据进行排序，降低数据排序的成本，降低cpu的消耗

-------

#### 补充：InnoDB引擎

InnoDB 是 MySQL 数据库管理系统中的一种存储引擎（Storage Engine）。存储引擎是负责管理数据库表的底层存储和检索的组件。MySQL 支持多种存储引擎，而 InnoDB 是其中一种广泛使用的存储引擎之一。

以下是 InnoDB 存储引擎的一些关键特性和特点：

1. **事务支持：** InnoDB 是一个支持事务的存储引擎。它遵循 ACID（原子性、一致性、隔离性、持久性）属性，确保在事务处理中数据的完整性和一致性。

2. **行级锁定：** InnoDB 使用行级锁定（row-level locking），这使得多个事务可以同时访问同一表的不同行，从而提高并发性能。

3. **外键约束：** InnoDB 提供了对外键的支持，可以保持表与表之间的关系完整性。这对于实施复杂的数据模型和关联表非常有用。

4. **MVCC（多版本并发控制）：** InnoDB 使用 MVCC 来管理事务的隔离性。这允许每个事务在并发环境中感知到一致性的数据视图，而不会受到其他事务的干扰。

5. **自动崩溃恢复：** InnoDB 提供自动崩溃恢复机制，确保在数据库发生崩溃时能够尽可能地还原到一致的状态。

6. **缓冲池和刷新：** InnoDB 使用缓冲池来缓存表数据和索引，以提高读取性能。它还通过“刷新”机制将脏数据（已修改但尚未写入磁盘的数据）写回磁盘。

7. **插入缓冲：** InnoDB 使用插入缓冲（Insert Buffer）来优化主键索引的插入操作，提高插入性能。

8. **独立表空间：** 每个 InnoDB 表都有一个独立的表空间，这使得对表的备份和恢复更为灵活。

由于其强大的事务支持、高并发性和其他先进的功能，InnoDB 成为了许多 MySQL 数据库中默认的存储引擎。然而，MySQL 也支持其他存储引擎，每个存储引擎都有其适用的场景和优势。



### 索引的底层数据结构了解过吗

Mysql的InnoDB引擎采用的是B+树的数据结构来存储索引

- 阶数更多，路径更短
- 磁盘读写代价B+树更低，非叶子节点只存储指针，叶子节点存储数据
- B+树便于扫库和区间查询，叶子节点是一个双向链表

### 什么是聚簇索引，什么是非聚簇索引？

#### 什么是聚集索引（聚簇索引），什么是二级索引（非聚集索引）

**聚集索引**：将数据存储与索引放到了一块，索引结构的叶子节点保存了很数据。特点是必须有，而且只有一个。

**二级索引**：将数据与索引分开存储，索引结构的叶子节点关联的是对应的主键。特点是可以存在多个。

聚集索引选取规则：

1. 如果存在主键，主键就是聚集索引
2. 如果不存在主键，将使用第一个唯一（UNIQUE）索引作为聚集索引
3. 如果表中没有主键，或没有合适的唯一索引，则InnoDB会自动生成一个row Id作为隐藏的聚集索引

#### 什么是回表查询

通过二级索引找到对应的主键值，再通过找到的主键值到聚簇索引中查找整行数据，这个过程就是回表

### #（待补充）什么是覆盖索引











## 结合商品种类来实现商品id的生成

结合商品种类来实现商品ID的生成，一个常见的方法是将类别信息编码到ID中。这样的ID通常包括几个部分，例如：

- **类别代码**：每个商品类别有一个唯一的代码。
- **时间戳**：表示商品录入系统的时间，有助于排序和确保唯一性。
- **序列号**：在同一时间戳内区分不同的商品。
- **随机数或哈希**：确保ID的唯一性并增加难以预测的特性。

下面是一个简单的Java示例，演示如何根据商品种类生成ID：

```java 
import java.time.Instant;
import java.util.concurrent.atomic.AtomicLong;

public class ProductIdGenerator {
    // 假设每个商品种类都有一个唯一的标识符，例如 "ELEC" 代表电子产品
    // 可以根据实际情况扩展为枚举或者从数据库动态加载
    private static final String ELECTRONICS_CODE = "ELEC";
    private static final String CLOTHING_CODE = "CLOT";
    // ... 其他商品种类代码

    // 序列号生成器，这里用AtomicLong来确保线程安全
    private AtomicLong sequenceGenerator = new AtomicLong();

    public String generateUniqueId(String categoryCode) {
        long timestampPart = Instant.now().toEpochMilli(); // 当前时间戳
        long sequencePart = sequenceGenerator.getAndIncrement(); // 序列号
        return categoryCode + timestampPart + sequencePart;
    }

    // 示例方法：生成电子产品类别的ID
    public String generateElectronicsId() {
        return generateUniqueId(ELECTRONICS_CODE);
    }

    // 示例方法：生成服装类别的ID
    public String generateClothingId() {
        return generateUniqueId(CLOTHING_CODE);
    }

    // ... 其他类别的ID生成方法
}

// 使用示例
public class Main {
    public static void main(String[] args) {
        ProductIdGenerator generator = new ProductIdGenerator();
        String electronicsId = generator.generateElectronicsId();
        String clothingId = generator.generateClothingId();

        System.out.println("Electronics ID: " + electronicsId);
        System.out.println("Clothing ID: " + clothingId);
    }
}
```

上面的代码定义了一个`ProductIdGenerator`类，它可以为不同的商品类别生成唯一的ID。这里使用了简单的字符串拼接来组合类别代码、时间戳和序列号。序列号使用`AtomicLong`生成，以保证线程安全。

注意，这个简单的实现假设了以下几点：

- 类别代码是预先定义好的，且每个类别都有唯一的代码。
- 系统的并发量不会在同一毫秒内超过`Long`类型的最大值，否则序列号会重复。
- 如果系统是分布式的，还需要考虑跨不同服务器/实例的唯一性。

这个方案是一个起点，实际应用中可能需要更复杂的逻辑，例如处理不同数据中心的ID生成，或者避免在多台机器上ID冲突等。需要根据实际的业务需求和系统架构来进一步优化和定制。



##  spring cloud gateway

在项目中是有一个网关服务，config包中是CorsConfig跨域配置，filter包中，是一个登陆校验拦截AuthFilter

逻辑是：

1. 获取当前的访问的Path 
   - 判断当前路径是不是服务调用的内部接口，如果是内部接口直接拦截
2. 判断用户是否已经登陆

   - 用户登陆，IP合法
   - 用户登陆，但是IP不合法
   - 用户未登录
3. 其他情况不需要登陆，放行
4. 还会在其中获取token，token会放在header中或者cookie中

配置文件

```yaml
spring:
  application:
    name: cloud-gateway
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848
    gateway:
      routes:    # 项目中routes 配置在nacos中 server-gateway-dev.yaml
         #路由的ID，没有固定规则但要求唯一，建议配合服务名
        - id: config_route 
          #匹配后提供服务的路由地址, 这里lb之后，跟的是要调用的服务名称
          uri: lb://nacos-provider-8002
           # 断言，路径相匹配的条件
          predicates:
            - Path=/routeconfig/rest/**      
```

此时，当id为config_route的路由规则匹配某个请求后，在调用该请求对应的服务时，就会从nacos注册中心自动发现服务，并在服务调用的时候实现负载均衡。

代码中的yaml文件：

```yml
spring:
  application:
    name: server-gateway
  profiles:
    active: dev
  cloud:
    nacos:
      discovery:
        server-addr: 120.46.174.115:8848
      config:
        server-addr: 120.46.174.115:8848
        file-extension: yaml
        shared-configs:
          - data-id: common.yaml
        timeout: 10000
# 网关会使用 配置中心里面哪些配置文件？
# common.yaml
# server-gateway-dev.yaml
```






## （待完成 ...  先把问题都解决）整个流程复盘一下 



1. 当请求进来之后， 先到nginx中，静态请求nginx直接处理，动态请求再交给后端的应用服务器，这里也可以减轻后端服务器的压力

2. 然后在nginx的配置文件中**http块**下的**upstream块**中配置负载均衡组，自定义好负载均衡组的名字，还有负载均衡策略（默认为轮询）如：

   ```nginx
   upstream backend {
       server backend-server1 weight=3;
       server backend-server2 weight=2;
       # 添加更多后端服务器...
   }
   ```

   

3. 然后在**http块**下面处理动态请求的**location块**中，添加 如proxy_pass  http://backend;

4. 请求然后再到spring cloud gateway进行请求的认证和转发， 其中spring.cloud.gateway.routes配置里的uri ：lb://service-product 表示要分发到这个服务上，并且lb（load balancer）表示要进行负载均衡

5. 如果没有显示的配置NFloadBalancerRuleClassName指定具体的负载均衡策略的话，那么就会使用默认的ZoneAoidanceRule区域敏感策略，帮我们选择一个服务实例

6. nacos是引入依赖，配置文件中，配置nacos的地址（服务中心和配置中心）spring.cloud.nacos.discovery.server-addr和spring.cloud.nacos.cofig.server-addr

7. 动态路由都会先web服务中，web服务通过远程调用，来获取其他服务中的数据，随后使用Thymeleaf模板引擎渲染HTML页面，并将这些动态生成的页面作为HTTP响应返回，

   - （待补充thymeleaf再细看商品详情的pdf，return “index”是什么意思）先进行了一个后端渲染 

8. （待完成，看商品具体的功能了） 商品服务有

   - 查看sku商品基本信息（包括sku商品的图片列表），
   - 查询sku商品所属spu中，包含的所有销售属性及销售属性值
   - 查询sku商品最新价格 （从数据库里查，因为查询的是最新的价格）
   - sku商品所属的三级分类信息等... 
   - 在SkuServiceImpl中的**上架**方法里要向**布隆过滤器**（**Redisson本身实现了基于Redis的布隆过滤器，可以非常方便的使用**）中添加skuId；同时在上架方法里要去发送**mq消息**，通知es要添加该商品的信息；在**下架**方法中，也要向es中发消息
   - 还要通过远程调用搜索服务，进行热度排名

9. rocketMq 见上文

10. （待补充）oss 方式使用MinIO

11. 搜索服务 通过远程调用商品服务，拿到

    - 根据skuId获取sku信息
    - 通过三级分类查询分类信息等
    - 还有两个消息消费者（商品上架和商品下架），通过监听，监听到消息后，调用如：searchService.lowerGoods，来进行es的上下架
    - 最终的查询是，结合关键字查询，过滤查询，分页查询，排序查询，聚合查询，高亮查询，指定结果字段，组合起来的DSL
    - 增加热度功能

    
    
    
    
    ## 比较印象深刻的问题
    
    项目启动的时候不报错，代码也没有动，但是跑着跑着就报错了，每次已重启就好了，后来发现是项目跑着跑着内存溢出，
    
    定位问题： 检查了网络是否正常（ping命令）；查看服务器cpu占用情况和内存的占用情况（top命令、free命令）；检查日志（tail命令），最后发现日志文件中报了堆溢出。
    
    接下来就要查找是哪个对象溢出了，用了jdk自带的内存快照工具，在jdk的bin目录下可以找到，然后将内存快照文件**dump文件**放进去分析（需要自己提前设置内存快照文件的生成），找到了对应的没有被及时回收的对象，通过调整该部分的代码解决了对应的问题。
    
    









